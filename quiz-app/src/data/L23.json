{
    "L23": {
        "title": "機器學習技術與應用",
        "sections": {
            "L231": {
                "title": "機器學習基礎數學",
                "sub_sections": {
                    "L23101": {
                        "title": "機率/統計之機器學習基礎應用",
                        "questions": [
                            {
                                "question_text": "在機器學習中，貝氏定理 (Bayes' Theorem) 主要用於什麼情境？",
                                "options": {
                                    "A": "計算數據的平均值與變異 (Variance)數 (Variance)",
                                    "B": "根據新的證據更新事件發生的機率 (計算後驗機率 (Posterior Probability))",
                                    "C": "進行線性迴歸 (Regression)模型的參數估計",
                                    "D": "判斷兩組數據的平均數 (Mean)是否有顯著差異"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>貝氏定理 (Bayes' Theorem) 描述了在已知某些條件的情況下，某一事件發生的機率。在機器學習中，它常被用於根據觀測到的新證據（數據）來更新我們對某個假設或事件發生機率的信念，即計算後驗機率 (Posterior Probability)。例如，在樸素貝氏分類器中，就是利用貝氏定理來預測樣本屬於特定類別的後驗機率。</p><p><strong>(A) 錯誤。</strong>計算數據的平均值與變異數屬於描述性統計 (Descriptive Statistics) 的範疇，用於總結數據的基本特徵，與貝氏定理直接更新機率的功能不同。</p><p><strong>(C) 錯誤。</strong>進行線性迴歸模型的參數估計通常使用最小平方法 (Least Squares Method) 或最大概似估計 (Maximum Likelihood Estimation) 等方法，而非直接應用貝氏定理來估計參數（雖然貝氏線性迴歸會用到貝氏方法，但題目問的是貝氏定理的主要用途）。</p><p><strong>(D) 錯誤。</strong>判斷兩組數據的平均數是否有顯著差異通常使用假設檢定中的 t-檢定 (t-test) 或 ANOVA 等統計方法，與貝氏定理更新機率的核心思想不同。</p></div>"
                            },
                            {
                                "question_text": "最大概似估計 (Maximum Likelihood Estimation) (Maximum Likelihood Estimation, MLE (Maximum Likelihood Estimation)) 的核心思想是什麼？",
                                "options": {
                                    "A": "選擇一組參數，使得在這些參數下，觀測到目前樣本數據的機率最小",
                                    "B": "選擇一組參數，使得在這些參數下，觀測到目前樣本數據的機率最大",
                                    "C": "選擇一組參數，使得模型的複雜度最低",
                                    "D": "選擇一組參數，使得模型的預測誤差最小"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>最大概似估計 (Maximum Likelihood Estimation, MLE) 是一種在給定觀測數據的情況下，估計模型參數的方法。其核心思想是：選擇一組能夠使得「已觀測到的樣本數據出現的機率（即概似函數值）」最大的參數值，作為參數的最佳估計。換句話說，我們認為最合理的參數估計，應該是那個最能解釋我們手中現有數據的參數。</p><p><strong>(A) 錯誤。</strong>這與MLE的核心思想相反。MLE旨在最大化觀測數據出現的機率，而非最小化。</p><p><strong>(C) 錯誤。</strong>選擇使得模型複雜度最低的原則通常與奧卡姆剃刀原則 (Occam's Razor) 或正規化 (Regularization) 方法相關，旨在防止過擬合，而非MLE的核心思想。MLE本身不直接考慮模型複雜度，而是專注於數據的概似程度。</p><p><strong>(D) 錯誤。</strong>選擇使得模型的預測誤差最小是許多機器學習模型訓練的目標，例如透過最小化損失函數（如均方誤差、交叉熵）來實現。雖然MLE的結果往往能導向預測誤差較小的模型，但其直接優化的目標是概似函數，而非預測誤差本身。</p></div>"
                            },
                            {
                                "question_text": "在假設檢定中，若p值 (p-value) 小於顯著水準 (significance level) (significance level, α)，通常會做出何種決策？",
                                "options": {
                                    "A": "接受虛無假設 (Null Hypothesis)",
                                    "B": "拒絕虛無假設 (Null Hypothesis)，接受對立假設 (Alternative Hypothesis)",
                                    "C": "增加樣本數重新進行檢定",
                                    "D": "p值 (p-value)與顯著水準 (significance level)無關"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>在假設檢定中，p值 (p-value) 是指在虛無假設 (Null Hypothesis, H0) 為真的前提下，觀測到目前樣本結果或更極端結果的機率。顯著水準 (significance level, α) 是一個預先設定的閾值（通常是0.05或0.01），代表我們願意承擔的犯第一型錯誤（即錯誤地拒絕一個真實的虛無假設）的最大機率。當計算出來的p值小於顯著水準α時，表示觀測到的數據與虛無假設的預期差異足夠大，這種差異不太可能是由隨機抽樣造成的。因此，我們有足夠的證據拒絕虛無假設，並接受對立假設 (Alternative Hypothesis, H1 或 Ha)。</p><p><strong>(A) 錯誤。</strong>接受虛無假設是在p值大於或等於顯著水準α時的決策，表示沒有足夠的證據拒絕虛無假設。</p><p><strong>(C) 錯誤。</strong>雖然增加樣本數可能會影響檢定結果（例如增加檢定力），但當p值小於α時，標準的決策是拒絕虛無假設，而不是立即要求增加樣本數。是否需要增加樣本數是另一個層面的考量，例如檢定力不足或結果不夠精確時。</p><p><strong>(D) 錯誤。</strong>p值與顯著水準密切相關，它們是做出統計決策的關鍵依據。決策規則就是比較p值和α的大小。</p></div>"
                            },
                            {
                                "question_text": "下列何種機率分佈常用於描述在固定時間間隔或空間區域內，某事件發生次數的機率？",
                                "options": {
                                    "A": "常態分佈 (Normal Distribution)",
                                    "B": "伯努利分佈 (Bernoulli Distribution)",
                                    "C": "二項分佈 (Binomial Distribution)",
                                    "D": "泊松分佈 (Poisson Distribution)"
                                },
                                "correct_answer": "D",
                                "solution": "<div><p><strong>(D) 正確。</strong>泊松分佈 (Poisson Distribution) 是一種離散機率分佈，用於描述在一個固定的時間間隔、空間區域、或指定的單位內，某一獨立事件發生的平均次數已知（通常用λ表示）的情況下，該事件實際發生k次的機率。例如，一小時內到達某服務台的顧客人數、一頁書中的錯字數量等，都可能服從泊松分佈。</p><p><strong>(A) 錯誤。</strong>常態分佈 (Normal Distribution)，也稱高斯分佈，是一種連續機率分佈，描述了許多自然現象中連續變量的分佈情況，其圖形呈鐘形。它與描述固定區間內事件發生次數的泊松分佈不同。</p><p><strong>(B) 錯誤。</strong>伯努利分佈 (Bernoulli Distribution) 是一種離散機率分佈，描述的是單次隨機試驗的結果，該試驗只有兩種可能的結果（例如成功/失敗，正面/反面），通常用0和1表示。它關注的是單次試驗，而非固定區間內的發生次數。</p><p><strong>(C) 錯誤。</strong>二項分佈 (Binomial Distribution) 是一種離散機率分佈，描述的是在n次獨立的伯努利試驗中，成功發生k次的機率，其中每次試驗成功的機率p保持不變。它關注的是固定次數試驗中的成功次數，而泊松分佈關注的是固定區間內的事件發生次數，且泊松分佈可以看作是當n很大、p很小時二項分佈的極限情況。</p></div>"
                            }
                        ]
                    },
                    "L23102": {
                        "title": "線性代數之機器學習基礎應用",
                        "questions": [
                            {
                                "question_text": "在機器學習中，特徵向量 (feature vector) 通常如何表示？",
                                "options": {
                                    "A": "一個純量 (scalar)",
                                    "B": "一個包含多個數值的一維陣列或向量",
                                    "C": "一個二維的數值表格 (矩陣)",
                                    "D": "一個高維的張量 (tensor)"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>在機器學習中，一個樣本或一個觀測對象通常由一組特徵 (features) 來描述。這些特徵的數值被組織成一個有序的列表或陣列，稱為特徵向量 (feature vector)。每個特徵向量代表數據集中的一個實例，向量中的每個元素對應一個特定的特徵值。因此，它是一個包含多個數值的一維陣列或向量。</p><p><strong>(A) 錯誤。</strong>一個純量 (scalar) 是一個單一的數值，它只能表示一個特徵，而特徵向量通常包含描述一個樣本的多個特徵。</p><p><strong>(C) 錯誤。</strong>一個二維的數值表格通常表示一個矩陣 (matrix)。在機器學習中，整個數據集（包含多個樣本的特徵向量）可以表示為一個矩陣，其中每一行（或每一列，取決於約定）是一個特徵向量。但單個樣本的特徵向量本身是一維的。</p><p><strong>(D) 錯誤。</strong>一個高維的張量 (tensor) 是向量和矩陣向更高維度的推廣。例如，彩色圖像可以表示為三維張量（高度 x 寬度 x 顏色通道）。雖然某些類型的數據（如圖像、影片）可以用張量表示，並且張量中的某個切片可能被視為特徵向量，但「特徵向量」本身最基礎和普遍的定義是一個一維的數值陣列。</p></div>"
                            },
                            {
                                "question_text": "計算兩個向量的點積 (dot product) 在機器學習中有何用途？",
                                "options": {
                                    "A": "判斷兩個向量是否正交 (垂直)",
                                    "B": "計算向量的長度或大小",
                                    "C": "作為神經網路中加權和計算的一部分",
                                    "D": "以上皆是"
                                },
                                "correct_answer": "D",
                                "solution": "<div><p><strong>(D) 正確。</strong>向量點積在機器學習中有多種重要用途：</p><p>- <strong>判斷兩個向量是否正交</strong>：如果兩個非零向量的點積為0，則它們互相正交（垂直）。這在理解特徵之間的關係或某些演算法（如PCA中的主成分）時很有用。</p><p>- <strong>計算向量的長度或大小（範數）</strong>：一個向量與其自身的點積的平方根即為該向量的歐幾里德長度（L2範數）。即 ||v|| = sqrt(v ⋅ v)。向量長度在距離計算、正規化等操作中非常關鍵。</p><p>- <strong>作為神經網路中加權和計算的一部分</strong>：在神經網路的每個神經元中，輸入特徵向量會與權重向量進行點積運算，然後加上一個偏置項，再通過活化函數。這個點積（加權和）是神經元計算的核心步驟。</p><p>因此，以上所有選項都是向量點積在機器學習中的正確用途。</p></div>"
                            },
                            {
                                "question_text": "奇異值分解 (Singular Value Decomposition, SVD) 是一種重要的矩陣分解技術，它在機器學習中的應用不包含下列何者？",
                                "options": {
                                    "A": "降維 (Dimensionality Reduction)，如主成分分析 (PCA) 的一種實現方式",
                                    "B": "推薦系統中的協同過濾",
                                    "C": "自然語言處理中的潛在語義分析 (LSA)",
                                    "D": "直接用於訓練深度卷積神經網路的卷積層"
                                },
                                "correct_answer": "D",
                                "solution": "<div><p><strong>(D) 正確。</strong>奇異值分解 (SVD) 是一種強大的矩陣分解技術，廣泛應用於數據科學和機器學習中。然而，它並不直接用於訓練深度卷積神經網路 (CNN) 的卷積層。CNN的卷積層是透過反向傳播演算法和梯度下降來學習其卷積核（權重）的，而不是直接使用SVD的結果作為卷積核。</p><p><strong>(A) 錯誤</strong>，這是SVD的應用。SVD是主成分分析 (PCA) 的一種數學基礎和實現方式。透過SVD可以找到數據的主要變異方向，從而進行降維。</p><p><strong>(B) 錯誤</strong>，這是SVD的應用。在推薦系統中，SVD常被用於協同過濾，例如透過分解用戶-物品評分矩陣來發現潛在特徵，進而預測用戶對未評分物品的偏好。</p><p><strong>(C) 錯誤</strong>，這是SVD的應用。在自然語言處理中，潛在語義分析 (Latent Semantic Analysis, LSA) 或潛在語義索引 (Latent Semantic Indexing, LSI) 利用SVD來分析詞彙-文檔矩陣，以捕捉詞彙和文檔之間的潛在語義關係，用於資訊檢索和文本分析。</p></div>"
                            },
                            {
                                "question_text": "在線性代數中，矩陣的特徵值 (eigenvalue) 與特徵向量 (eigenvector) 描述了矩陣在特定方向上的什麼特性？",
                                "options": {
                                    "A": "矩陣的行數與列數",
                                    "B": "矩陣經過線性變換後，特徵向量的方向不變，僅在長度上以特徵值為比例進行縮放",
                                    "C": "矩陣的轉置與逆矩陣",
                                    "D": "矩陣的行列式值"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>對於一個給定的方陣A，如果存在一個非零向量v和一個純量λ，使得Av = λv成立，那麼λ就被稱為矩陣A的一個特徵值 (eigenvalue)，而v則被稱為對應於特徵值λ的特徵向量 (eigenvector)。這個等式的幾何意義是，當矩陣A對特徵向量v進行線性變換時，其效果僅僅是將向量v在其原始方向上進行縮放，縮放的比例因子就是特徵值λ。特徵向量的方向在變換後保持不變（或者方向相反，如果特徵值為負）。</p><p><strong>(A) 錯誤。</strong>矩陣的行數與列數描述的是矩陣的維度或大小，與特徵值和特徵向量所描述的線性變換特性不同。</p><p><strong>(C) 錯誤。</strong>矩陣的轉置 (transpose) 是將矩陣的行和列互換得到的新矩陣。逆矩陣 (inverse matrix) 是指與原矩陣相乘結果為單位矩陣的矩陣（如果存在）。這些是矩陣的屬性或相關矩陣，但不是特徵值和特徵向量直接描述的特性。</p><p><strong>(D) 錯誤。</strong>矩陣的行列式 (determinant) 是一個與方陣相關的純量值，它可以提供關於矩陣是否可逆、線性變換對面積或體積的影響等信息，但它本身不是特徵值或特徵向量所描述的「特定方向上的特性」。特徵值的乘積等於行列式值（對於某些類型的矩陣），但這是一個間接關係。</p></div>"
                            }
                        ]
                    },
                    "L23103": {
                        "title": "數值優化技術與方法",
                        "questions": [
                            {
                                "question_text": "梯度下降 (Gradient Descent)法 (Gradient Descent) 是一種常用的優化演算法，其更新參數的主要依據是什麼？",
                                "options": {
                                    "A": "損失函數 (Loss Function)的二階導數 (Hessian矩陣 (Hessian matrix))",
                                    "B": "損失函數 (Loss Function)對於模型參數的梯度 (一階導數)",
                                    "C": "隨機選擇一個方向進行更新",
                                    "D": "損失函數 (Loss Function)本身的值"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>梯度下降法 (Gradient Descent) 是一種迭代優化演算法，用於尋找函數的局部最小值。在機器學習中，這個函數通常是損失函數 (Loss Function)，而我們希望找到使損失函數最小化的模型參數。梯度是函數在某一點上變化最快（上升最快）的方向，其負方向則是函數下降最快的方向。因此，梯度下降法透過計算損失函數對於模型參數的梯度（即一階偏導數向量），然後沿著梯度的負方向按一定步長（學習率）更新參數，從而逐步逼近損失函數的最小值點。</p><p><strong>(A) 錯誤。</strong>損失函數的二階導數（Hessian矩陣）被用於更高級的優化方法，如牛頓法 (Newton's method) 或擬牛頓法 (Quasi-Newton methods)。這些方法可以利用曲率信息來加速收斂，但梯度下降法本身主要依賴一階導數（梯度）。</p><p><strong>(C) 錯誤。</strong>隨機選擇一個方向進行更新的方法，如隨機搜索 (Random Search)，通常效率較低，且不保證能找到最優解。梯度下降法是基於梯度的確定性方向（或其估計）進行更新的。</p><p><strong>(D) 錯誤。</strong>雖然損失函數本身的值用於評估當前參數的好壞，但參數更新的方向是由梯度決定的，而不是直接由損失函數的值決定的。梯度指明了如何調整參數以最大程度地減少損失。</p></div>"
                            },
                            {
                                "question_text": "在梯度下降 (Gradient Descent)法 (Gradient Descent)中，學習率 (learning rate) 的選擇有何影響？",
                                "options": {
                                    "A": "學習率 (learning rate)越大，收斂速度一定越快且結果越好",
                                    "B": "學習率 (learning rate)越小，收斂速度一定越慢但結果越好",
                                    "C": "學習率 (learning rate)過大可能導致震盪或無法收斂；學習率 (learning rate)過小可能導致收斂速度過慢",
                                    "D": "學習率 (learning rate)對模型訓練沒有實質影響"
                                },
                                "correct_answer": "C",
                                "solution": "<div><p><strong>(C) 正確。</strong>學習率 (learning rate) 是梯度下降法中的一個關鍵超參數，它控制了每一步參數更新的幅度（即沿著梯度負方向移動的距離）。學習率的選擇對模型的訓練過程和最終性能有顯著影響：</p><p>-   <strong>學習率過大</strong>：如果學習率設置得太大，參數更新的步長可能過大，導致在損失函數的谷底附近來回震盪，甚至越過最小值點，使得損失函數不降反升，最終無法收斂到最優解。</p><p>-   <strong>學習率過小</strong>：如果學習率設置得太小，參數更新的步長會非常緩慢，導致模型收斂到最優解的速度非常慢，需要更多的迭代次數，增加了訓練時間。</p><p>因此，選擇一個合適的學習率至關重要，它需要在收斂速度和收斂穩定性之間取得平衡。</p><p><strong>(A) 錯誤。</strong>學習率越大，收斂速度不一定越快，如果過大反而可能導致不收斂或震盪，結果也可能更差。</p><p><strong>(B) 錯誤。</strong>學習率越小，收斂速度確實會變慢，但並不保證結果一定越好。過小的學習率可能導致模型陷入局部最小值，或者需要極長的訓練時間才能達到一個好的解。</p><p><strong>(D) 錯誤。</strong>學習率對模型訓練有非常實質的影響，是梯度下降法能否成功找到最優解的關鍵因素之一。</p></div>"
                            },
                            {
                                "question_text": "隨機梯度下降 (Gradient Descent)法 (Stochastic Gradient Descent) (Stochastic Gradient Descent, SGD (Stochastic Gradient Descent)) 與批次梯度下降 (Gradient Descent)法 (Batch Gradient Descent) 的主要區別是什麼？",
                                "options": {
                                    "A": "SGD (Stochastic Gradient Descent) 使用整個訓練集的梯度來更新參數，BGD (Batch Gradient Descent) 使用單一樣本",
                                    "B": "SGD (Stochastic Gradient Descent) 每次使用單一或小批量樣本的梯度來更新參數，BGD (Batch Gradient Descent) 使用整個訓練集",
                                    "C": "SGD (Stochastic Gradient Descent) 的計算成本通常高於 BGD (Batch Gradient Descent)",
                                    "D": "BGD (Batch Gradient Descent) 的收斂路徑通常比 SGD (Stochastic Gradient Descent) 更不穩定"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>梯度下降法有幾種變體，主要區別在於每次參數更新時使用的數據量：</p><p>-   <strong>批次梯度下降法 (Batch Gradient Descent, BGD)</strong>：在每次參數更新時，使用整個訓練集的數據來計算損失函數的梯度。這種方法計算的梯度更準確，收斂路徑更穩定，但當訓練集非常大時，每次迭代的計算成本很高，且可能難以處理無法一次性載入記憶體的數據集。</p><p>-   <strong>隨機梯度下降法 (Stochastic Gradient Descent, SGD)</strong>：在每次參數更新時，僅使用訓練集中的一個隨機選擇的樣本來計算梯度。這種方法每次迭代的計算成本非常低，更新速度快，並且其隨機性有助於跳出局部最小值。然而，由於每次只用一個樣本，梯度估計的變異較大，導致收斂路徑較為震盪。</p><p>-   <strong>小批量梯度下降法 (Mini-batch Gradient Descent)</strong>：這是介於BGD和SGD之間的一種折衷方法。它在每次參數更新時，使用訓練集中的一小部分隨機樣本（一個mini-batch）來計算梯度。這種方法結合了BGD的穩定性和SGD的效率，是目前深度學習中最常用的梯度下降變體。</p><p>題目中的SGD指的是使用單一或小批量樣本，與BGD使用整個訓練集形成對比。</p><p><strong>(A) 錯誤。</strong>這描述反了。BGD使用整個訓練集，SGD使用單一（或小批量）樣本。</p><p><strong>(C) 錯誤。</strong>SGD（尤其是單樣本SGD）每次迭代的計算成本遠低於BGD，因為它只需要處理一個樣本而不是整個訓練集。即使是Mini-batch SGD，其計算成本也通常低於BGD（除非mini-batch大小等於整個訓練集大小）。</p><p><strong>(D) 錯誤。</strong>由於BGD使用整個訓練集的梯度，其收斂路徑通常比SGD更平滑、更穩定。SGD由於每次使用隨機樣本估計梯度，其收斂路徑會更加震盪和不穩定。</p></div>"
                            },
                            {
                                "question_text": "在機器學習中，損失函數 (Loss Function) 的主要作用是什麼？",
                                "options": {
                                    "A": "衡量模型預測的準確程度，數值越大代表模型越好",
                                    "B": "衡量模型預測值與真實值之間的差異，優化的目標是最小化損失函數 (Loss Function)",
                                    "C": "用於對輸入數據進行特徵提取 (Feature Extraction)",
                                    "D": "決定模型架構的複雜度"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>損失函數（也稱為成本函數或目標函數）在機器學習中扮演著核心角色。它的主要作用是量化模型預測結果與實際真實值（標籤）之間的差異或「損失」。一個好的模型應該使其預測盡可能接近真實值，因此損失函數的值越小，通常表示模型的性能越好。在模型訓練過程中，優化的目標就是調整模型的參數（例如神經網路的權重），以最小化損失函數的值。</p><p><strong>(A) 錯誤。</strong>損失函數衡量的是模型的「錯誤」或「損失」程度，因此其數值越小代表模型越好，而非越大越好。準確度 (Accuracy) 等指標才是數值越大代表模型越好。</p><p><strong>(C) 錯誤。</strong>特徵提取 (Feature Extraction) 是數據預處理或模型架構的一部分（例如CNN中的卷積層），旨在從原始數據中抽取出更有代表性、更能區分不同類別的特徵。損失函數是用於評估和指導模型學習這些特徵（以及後續的分類/迴歸任務）的好壞，而不是直接進行特徵提取。</p><p><strong>(D) 錯誤。</strong>決定模型架構的複雜度（例如神經網路的層數、神經元數量，或決策樹的深度等）是模型設計階段的任務，通常由開發者根據問題特性、數據量和計算資源等因素來決定。損失函數本身不決定模型架構的複雜度，但模型的複雜度會影響其最小化損失函數的能力以及是否容易過擬合。</p></div>"
                            }
                        ]
                    }
                }
            },
            "L232": {
                "title": "機器學習與深度學習",
                "sub_sections": {
                    "L23201": {
                        "title": "機器學習原理與技術",
                        "questions": [
                            {
                                "question_text": "下列何者是監督式學習 (Supervised Learning) 的主要特點？",
                                "options": {
                                    "A": "訓練數據沒有標籤，模型需要自行找出數據中的結構",
                                    "B": "訓練數據包含輸入特徵以及對應的正確輸出標籤",
                                    "C": "模型透過與環境互動，學習最大化獎勵的策略",
                                    "D": "主要用於數據降維 (Dimensionality Reduction)與視覺化"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>監督式學習 (Supervised Learning) 是機器學習的一大類別，其核心特點是使用帶有標籤 (labeled) 的訓練數據。這意味著提供給模型的每個輸入樣本（由一組特徵描述）都配有一個已知的、正確的輸出結果或目標值（即標籤）。模型的任務是學習從輸入特徵到輸出標籤之間的映射關係，以便能夠對新的、未見過的輸入數據進行準確的預測。常見的監督式學習任務包括分類 (Classification) 和迴歸 (Regression)。</p><p><strong>(A) 錯誤。</strong>訓練數據沒有標籤，模型需要自行找出數據中的內在結構、模式或分組，這描述的是非監督式學習 (Unsupervised Learning) 的主要特點，例如分群 (Clustering) 或關聯規則學習 (Association Rule Learning)。</p><p><strong>(C) 錯誤。</strong>模型透過與環境互動，並根據收到的獎勵或懲罰來學習最佳行動策略，以最大化長期累積獎勵，這描述的是強化學習 (Reinforcement Learning) 的主要特點。</p><p><strong>(D) 錯誤。</strong>數據降維 (Dimensionality Reduction)，如主成分分析 (PCA)，以及數據視覺化，通常屬於非監督式學習的範疇，因為它們旨在探索數據的結構或以更簡潔的方式表示數據，而不一定需要預先定義的標籤。雖然監督式方法也可以用於特徵選擇（一種形式的降維），但「主要用於數據降維與視覺化」更符合非監督式學習的描述。</p></div>"
                            },
                            {
                                "question_text": "當一個機器學習模型在訓練數據上表現良好，但在未見過的測試數據上表現很差時，這種現象稱為什麼？",
                                "options": {
                                    "A": "欠擬合 (Underfitting)",
                                    "B": "過擬合 (Overfitting)",
                                    "C": "偏差 (Bias)",
                                    "D": "變異 (Variance)"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>過擬合 (Overfitting) 是指機器學習模型在訓練數據上學到了過多的細節和噪聲，導致其對訓練數據的擬合非常好（例如，訓練誤差很低），但在新的、未見過的數據（如測試數據或實際應用中的數據）上表現不佳（例如，測試誤差很高）。模型失去了泛化能力，無法很好地適應新的數據。這通常發生在模型過於複雜（相對於數據量和數據的真實複雜度而言）或者訓練時間過長的情況下。</p><p><strong>(A) 錯誤。</strong>欠擬合 (Underfitting) 是指模型過於簡單，無法捕捉到數據中的基本模式和規律，導致其在訓練數據上和測試數據上都表現不佳。模型沒有充分學習數據的特性。</p><p><strong>(C) 錯誤。</strong>偏差 (Bias) 是指模型預測值的期望與真實值之間的差異，衡量的是模型的擬合能力。高偏差通常意味著模型欠擬合，無法很好地捕捉數據的真實關係。雖然過擬合與偏差和變異有關（通常是低偏差、高變異），但題目描述的現象直接指的是過擬合。</p><p><strong>(D) 錯誤。</strong>變異 (Variance) 是指模型在不同訓練數據集上訓練時，其預測結果的變化程度，衡量的是模型對訓練數據變化的敏感性。高變異通常意味著模型過擬合，對訓練數據中的噪聲過於敏感。與偏差類似，雖然過擬合通常伴隨著高變異，但題目描述的現象本身被稱為過擬合。</p></div>"
                            },
                            {
                                "question_text": "交叉驗證 (Cross-Validation) 在機器學習中的主要目的是什麼？",
                                "options": {
                                    "A": "增加訓練數據的數量",
                                    "B": "更可靠地評估模型的泛化能力，並輔助模型選擇與超參數調整",
                                    "C": "加速模型的訓練過程",
                                    "D": "降低模型的複雜度"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>交叉驗證 (Cross-Validation) 是一種評估機器學習模型泛化能力的統計方法。它將原始數據集劃分為多個子集（或稱為「摺」，folds），然後輪流使用其中一個子集作為驗證集（用於評估模型），其餘子集作為訓練集（用於訓練模型）。這個過程重複多次，每次使用不同的子集作為驗證集。最後，將多次評估的結果（例如準確率、誤差）平均起來，得到一個更穩定、更可靠的模型性能估計。交叉驗證的主要目的包括：</p><p>-   <strong>更可靠地評估模型的泛化能力</strong>：相比於單次劃分訓練集和測試集，交叉驗證可以減少因數據劃分的隨機性帶來的評估偏差，從而更準確地反映模型在未見數據上的表現。</p><p>-   <strong>輔助模型選擇與超參數調整</strong>：在比較不同模型或調整模型超參數時，可以使用交叉驗證來選擇在驗證集上平均表現最好的模型或超參數組合，以避免過擬合到特定的驗證集。</p><p><strong>(A) 錯誤。</strong>交叉驗證本身並不增加訓練數據的總量，它只是更有效地利用現有的數據進行模型訓練和評估。數據增強 (Data Augmentation) 等技術才是用於增加訓練數據數量的方法。</p><p><strong>(C) 錯誤。</strong>交叉驗證通常會增加總體的計算時間，因為模型需要被訓練和評估多次（例如，k-摺交叉驗證需要訓練k次模型）。它並不能加速單個模型的訓練過程。</p><p><strong>(D) 錯誤。</strong>降低模型的複雜度通常是透過正規化 (Regularization)、剪枝 (Pruning，如決策樹) 或選擇更簡單的模型架構來實現的，目的是防止過擬合。交叉驗證本身是評估工具，雖然其結果可能引導我們選擇一個複雜度較低的模型（如果該模型泛化能力更好），但它不直接降低模型的複雜度。</p></div>"
                            },
                            {
                                "question_text": "偏差 (Bias)-變異 (Variance)權衡 (Bias-Variance Tradeoff) 描述了機器學習模型中的哪種關係？",
                                "options": {
                                    "A": "模型的偏差 (Bias)越高，變異 (Variance)通常也越高",
                                    "B": "模型的偏差 (Bias)越低，變異 (Variance)通常也越低",
                                    "C": "降低模型的偏差 (Bias)可能會導致變異 (Variance)增加，反之亦然，需要在兩者間取得平衡",
                                    "D": "偏差 (Bias)與變異 (Variance)是完全獨立的，不存在權衡關係"
                                },
                                "correct_answer": "C",
                                "solution": "<div><p><strong>(C) 正確。</strong>偏差-變異權衡 (Bias-Variance Tradeoff) 是機器學習中一個核心概念，描述了模型預測誤差的兩個主要來源之間的關係：</p><p>-   <strong>偏差 (Bias)</strong>：指模型預測值的期望與真實值之間的差異。高偏差意味著模型對數據的假設過於簡單，未能捕捉到數據的真實規律，導致欠擬合 (Underfitting)。</p><p>-   <strong>變異 (Variance)</strong>：指模型在不同訓練數據集上訓練時，其預測結果的變化程度或不穩定性。高變異意味著模型對訓練數據中的噪聲或隨機波動過於敏感，導致過擬合 (Overfitting)。</p><p>偏差-變異權衡指出，通常情況下，當我們試圖降低模型的偏差時（例如，使用更複雜的模型），其變異可能會增加；反之，當我們試圖降低模型的變異時（例如，使用更簡單的模型或正規化），其偏差可能會增加。因此，在模型選擇和訓練過程中，需要在偏差和變異之間找到一個平衡點，以最小化總體的預期泛化誤差。</p><p><strong>(A) 錯誤。</strong>高偏差（欠擬合）的模型通常對數據不敏感，因此其變異可能較低。而高變異（過擬合）的模型通常能夠很好地擬合訓練數據，偏差可能較低。</p><p><strong>(B) 錯誤。</strong>理想情況下我們希望偏差和變異都低，但實際中兩者往往存在權衡關係，很難同時達到最低。</p><p><strong>(D) 錯誤。</strong>偏差與變異並非完全獨立，它們之間存在著此消彼長的權衡關係，是影響模型泛化能力的兩個重要且相互關聯的因素。</p></div>"
                            },
                            {
                                "question_text": "集成學習 (Ensemble Learning) 方法，如隨機森林和梯度提升樹，其核心思想是什麼？",
                                "options": {
                                    "A": "只使用單一且最強大的模型進行預測",
                                    "B": "將多個弱學習器 (weak learners) 組合起來，形成一個更強大、更穩健的學習器",
                                    "C": "專注於簡化模型的結構以提高解釋性",
                                    "D": "主要用於無監督學習任務"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>集成學習 (Ensemble Learning) 是一種機器學習範式，其核心思想是「三個臭皮匠，勝過一個諸葛亮」。它不依賴於單一的、可能存在缺陷的模型，而是將多個相對簡單或性能較弱的學習器（稱為基學習器或弱學習器，weak learners）的預測結果以某種方式組合起來，從而創建一個整體性能更好、更穩健、泛化能力更強的強學習器 (strong learner)。常見的集成學習方法包括Bagging（如隨機森林Random Forest）、Boosting（如梯度提升樹Gradient Boosting Trees, AdaBoost）和Stacking等。</p><p><strong>(A) 錯誤。</strong>這與集成學習的思想相反。集成學習正是為了避免過度依賴單一模型，特別是當單一最強模型可能很難找到或容易過擬合時。</p><p><strong>(C) 錯誤。</strong>雖然某些集成方法（如隨機森林中的特徵重要性）可以提供一定的可解釋性，但集成學習的主要目標通常是提升預測性能和模型的穩健性，而不是簡化模型結構以提高解釋性。事實上，集成模型通常比單個基學習器更複雜，解釋性也可能更差。</p><p><strong>(D) 錯誤。</strong>集成學習方法主要應用於監督式學習任務（分類和迴歸），例如隨機森林和梯度提升樹都是監督式學習演算法。雖然集成思想也可以應用於非監督式學習（例如集成聚類），但其最典型和廣泛的應用是在監督式學習中。</p></div>"
                            },
                            {
                                "question_text": "在類神經網路中，活化函數 (Activation Function) 的主要作用是什麼？",
                                "options": {
                                    "A": "對輸入數據進行標準化",
                                    "B": "為神經元引入非線性，使得網路能夠學習更複雜的模式",
                                    "C": "計算損失函數的值",
                                    "D": "初始化網路的權重參數"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>活化函數 (Activation Function) 是類神經網路（或深度學習模型）中神經元的一個關鍵組成部分。在神經元接收到所有輸入的加權和（再加上偏置項）之後，這個結果會被傳遞給活化函數。活化函數的主要作用是引入非線性 (non-linearity) 到網路中。如果沒有非線性活化函數，多層神經網路本質上仍然是一個線性模型，其表達能力將非常有限，無法學習和表示複雜的數據模式（例如圖像識別、自然語言理解中的複雜關係）。常見的非線性活化函數包括 Sigmoid、Tanh、ReLU (Rectified Linear Unit) 及其變體等。這些函數使得神經網路能夠逼近任意複雜的非線性函數，從而處理現實世界中的複雜問題。</p><p><strong>(A) 錯誤。</strong>對輸入數據進行標準化 (Standardization) 或歸一化 (Normalization) 是數據預處理的一個步驟，目的是將數據調整到一個合適的範圍（例如均值為0，標準差為1），以幫助模型訓練的穩定性和收斂速度。這通常在數據進入網路之前完成，而不是活化函數的主要作用。</p><p><strong>(C) 錯誤。</strong>計算損失函數 (Loss Function) 的值是在模型進行預測之後，用於衡量預測結果與真實標籤之間的差異。損失函數指導模型的優化過程，但它不是活化函數本身的功能。</p><p><strong>(D) 錯誤。</strong>初始化網路的權重參數是模型訓練開始前的一個重要步驟，目的是為權重賦予初始值。權重初始化的好壞會影響模型的訓練效果，但這與活化函數的作用不同。活化函數是在前向傳播過程中對神經元的輸出進行非線性變換。</p></div>"
                            },
                            {
                                "question_text": "反向傳播演算法 (Backpropagation) 在深度學習模型訓練中的核心功能是什麼？",
                                "options": {
                                    "A": "對輸入圖像進行特徵提取",
                                    "B": "高效地計算損失函數對於網路中所有權重參數的梯度，以便進行權重更新",
                                    "C": "隨機初始化神經網路的權重",
                                    "D": "決定神經網路的層數與每層的神經元數量"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>反向傳播演算法 (Backpropagation) 是訓練多層神經網路（深度學習模型）的核心演算法。在模型進行一次前向傳播 (forward pass) 並計算出預測值與損失函數值之後，反向傳播演算法會從輸出層開始，逐層向後計算損失函數對於網路中每一個權重 (weight) 和偏置 (bias) 參數的梯度（即偏導數）。這些梯度指明了每個參數應該如何調整才能使損失函數減小。一旦計算出所有梯度，就可以使用梯度下降法（或其變體，如SGD, Adam等）來更新網路的參數。反向傳播的關鍵在於它利用了微積分中的鏈式法則 (chain rule)，能夠高效地計算這些梯度，即使對於非常深和複雜的網路也是如此。</p><p><strong>(A) 錯誤。</strong>對輸入圖像進行特徵提取是卷積神經網路 (CNN) 等模型在前向傳播過程中的一部分功能，例如卷積層和池化層會自動學習和提取圖像特徵。反向傳播是發生在特徵提取和預測之後，用於計算梯度以更新模型參數的過程。</p><p><strong>(C) 錯誤。</strong>隨機初始化神經網路的權重是在模型訓練開始之前進行的一個重要步驟，目的是打破對稱性並幫助模型開始學習。反向傳播是在權重初始化之後，用於在訓練過程中調整這些權重。</p><p><strong>(D) 錯誤。</strong>決定神經網路的層數與每層的神經元數量是模型架構設計的一部分，通常由人工設計或透過神經架構搜索 (Neural Architecture Search, NAS) 等技術來確定。反向傳播演算法本身不決定網路的架構，而是在給定架構下訓練模型參數。</p></div>"
                            },
                            {
                                "question_text": "卷積神經網路 (Convolutional Neural Network, CNN) 特別擅長處理哪種類型的數據？其核心組件通常包含哪些層？",
                                "options": {
                                    "A": "時間序列數據；循環層、注意力層",
                                    "B": "表格數據；全連接層、決策樹層",
                                    "C": "圖像或網格狀數據；卷積層、池化層、全連接層",
                                    "D": "文字數據；嵌入層、LSTM層"
                                },
                                "correct_answer": "C",
                                "solution": "<div><p><strong>(C) 正確。</strong>卷積神經網路 (Convolutional Neural Network, CNN 或 ConvNet) 是一種特殊設計的深度學習模型，因其在處理具有網格狀拓撲結構的數據方面表現出色而聞名，其中最典型的應用就是圖像處理和電腦視覺任務。CNN的核心思想是利用卷積運算來自動學習和提取數據中的局部空間層次特徵。其核心組件通常包括：</p><p>- <strong>卷積層 (Convolutional Layer)</strong>：透過可學習的濾波器（卷積核）對輸入數據進行卷積操作，以提取局部特徵，如邊緣、角點、紋理等。</p><p>- <strong>池化層 (Pooling Layer)</strong>：通常位於卷積層之後，用於降低特徵圖的空間維度（下採樣），減少計算量，並增強模型的平移不變性。常見的池化操作有最大池化 (Max Pooling) 和平均池化 (Average Pooling)。</p><p>- <strong>全連接層 (Fully Connected Layer)</strong>：在經過多個卷積層和池化層提取特徵後，通常會將特徵圖展平並連接到一個或多個全連接層，用於進行最終的分類或迴歸預測（類似於傳統的多層感知器）。</p><p><strong>(A) 錯誤。</strong>時間序列數據（如語音、股票價格）通常更適合使用循環神經網路 (RNN) 或其變體（如LSTM, GRU）以及注意力機制來處理，因為這些模型能夠捕捉序列中的時間依賴性。雖然CNN有時也用於時間序列（例如一維CNN），但其最擅長的領域是圖像等網格數據。</p><p><strong>(B) 錯誤。</strong>表格數據（結構化數據）通常使用傳統的機器學習演算法（如決策樹、隨機森林、梯度提升樹、支持向量機）或簡單的全連接神經網路（多層感知器）來處理。決策樹層不是標準CNN的組件。</p><p><strong>(D) 錯誤。</strong>文字數據（自然語言處理任務）傳統上常使用RNN、LSTM、GRU等模型，近年來Transformer模型（基於自注意力機制）取得了巨大成功。嵌入層 (Embedding Layer) 用於將文字轉換為向量表示，是處理文字數據的常見初始步驟，但CNN本身的核心組件不直接包含LSTM層。</p></div>"
                            },
                            {
                                "question_text": "循環神經網路 (Recurrent Neural Network, RNN) 為何適合處理序列數據（如文字、語音）？",
                                "options": {
                                    "A": "因為其網路結構中不包含任何循環連接",
                                    "B": "因為其隱藏層的狀態可以在不同時間步之間傳遞，從而捕捉序列中的時間依賴性",
                                    "C": "因為它只能處理固定長度的輸入序列",
                                    "D": "因為它不需要進行反向傳播訓練"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>循環神經網路 (Recurrent Neural Network, RNN) 的核心特點是其網路結構中包含循環連接（或稱為反饋迴路）。這使得網路的隱藏層狀態不僅依賴於當前時間步的輸入，還依賴於前一個時間步的隱藏層狀態。這種「記憶」機制允許信息在序列的不同時間步之間傳遞和保持，從而使RNN能夠有效地捕捉和學習序列數據（如文字、語音、時間序列數據等）中的時間依賴關係和上下文信息。例如，在處理一個句子時，RNN可以利用前面詞語的信息來理解當前詞語的含義。</p><p><strong>(A) 錯誤。</strong>這與RNN的定義相反。RNN的關鍵就在於其結構中存在循環連接，使得信息可以在時間步之間流動。</p><p><strong>(C) 錯誤。</strong>雖然傳統的簡單RNN在處理非常長的序列時可能會遇到梯度消失或梯度爆炸的問題，並且在實際應用中通常需要將序列截斷或填充到固定長度進行批次處理，但RNN的設計理念是能夠處理可變長度的序列。更高級的RNN變體，如長短期記憶網路 (LSTM) 和門控循環單元 (GRU)，被設計用來更好地處理長序列依賴問題。</p><p><strong>(D) 錯誤。</strong>RNN（包括其變體LSTM、GRU等）通常也是透過反向傳播演算法進行訓練的，不過是其適用於序列數據的變體，稱為隨時間反向傳播 (Backpropagation Through Time, BPTT)。BPTT將循環網路在時間上展開成一個深層的前饋網路，然後應用標準的反向傳播來計算梯度並更新權重。</p></div>"
                            },
                            {
                                "question_text": "Transformer 模型近年來在自然語言處理領域取得了巨大成功，其核心機制是什麼？",
                                "options": {
                                    "A": "卷積運算 (Convolution)",
                                    "B": "循環連接 (Recurrent Connection)",
                                    "C": "注意力機制 (Attention Mechanism)，特別是自注意力機制 (Self-Attention)",
                                    "D": "K-均值分群 (K-Means Clustering)"
                                },
                                "correct_answer": "C",
                                "solution": "<div><p><strong>(C) 正確。</strong>Transformer 模型（由 Vaswani 等人在論文 \"Attention Is All You Need\" 中提出）已經成為自然語言處理 (NLP) 領域的革命性架構，並在機器翻譯、文本生成、問答系統等眾多任務上取得了最先進的成果。其核心創新和成功的關鍵在於完全拋棄了傳統的循環連接 (RNN) 和卷積運算 (CNN) 作為主要的序列處理單元，而是完全依賴於注意力機制 (Attention Mechanism)，特別是其中的自注意力機制 (Self-Attention Mechanism)。自注意力機制允許模型在處理序列中的每個元素（例如一個詞）時，能夠同時關注到序列中所有其他元素，並根據它們之間的相關性來計算該元素的表示，從而有效地捕捉長距離依賴關係，並且能夠並行處理序列中的所有元素，大大提高了訓練效率。</p><p><strong>(A) 錯誤。</strong>雖然卷積運算 (Convolution) 在某些NLP模型（如文本CNN）中被用於提取局部特徵，但Transformer模型的核心並不是卷積運算。Transformer的設計初衷之一就是擺脫對卷積和循環的依賴。</p><p><strong>(B) 錯誤。</strong>循環連接 (Recurrent Connection) 是RNN及其變體（如LSTM, GRU）的核心特徵，用於按順序處理序列數據並捕捉時間依賴性。Transformer模型的一個主要突破就是不再使用循環連接，從而克服了RNN在處理長序列時的梯度消失/爆炸問題以及難以並行計算的缺點。</p><p><strong>(D) 錯誤。</strong>K-均值分群 (K-Means Clustering) 是一種非監督式學習演算法，用於將數據自動分組，與Transformer這種用於序列建模的監督式（或自監督式）深度學習架構的核心機制無關。</p></div>"
                            }
                        ]
                    },
                    "L23202": {
                        "title": "常見機器學習演算法",
                        "questions": [
                            {
                                "question_text": "下列哪種演算法常用於解決二元分類 (Classification)問題，並透過一個S型函數 (Sigmoid function) 將線性輸出轉換 (Transform)為機率值？",
                                "options": {
                                    "A": "線性迴歸 (Linear Regression)",
                                    "B": "K-均值分群 (K-Means Clustering)",
                                    "C": "羅吉斯迴歸 (Logistic Regression)",
                                    "D": "主成分分析 (Principal Component Analysis)"
                                },
                                "correct_answer": "C",
                                "solution": "<div><p><strong>(C) 正確。</strong>羅吉斯迴歸 (Logistic Regression) 是一種廣泛應用於解決二元分類問題（即預測結果只有兩個類別，例如是/否、成功/失敗）的統計學習方法。儘管其名稱中帶有「迴歸」，但它本質上是一個分類演算法。羅吉斯迴歸首先計算輸入特徵的線性組合（類似於線性迴歸），然後將這個線性輸出透過一個S型函數（Sigmoid function，也稱為Logistic function）映射到0到1之間的值。這個值可以被解釋為樣本屬於某個正類別的機率。如果機率大於某個閾值（通常是0.5），則預測為正類別，否則預測為負類別。</p><p><strong>(A) 錯誤。</strong>線性迴歸 (Linear Regression) 主要用於解決迴歸問題，即預測一個連續的數值輸出，而不是分類問題。它直接輸出特徵的線性組合，不使用S型函數將其轉換為機率。</p><p><strong>(B) 錯誤。</strong>K-均值分群 (K-Means Clustering) 是一種非監督式學習演算法，用於將數據自動分組成K個不同的群體 (clusters)，它不需要標籤數據，也不是用於二元分類或輸出機率值。</p><p><strong>(D) 錯誤。</strong>主成分分析 (Principal Component Analysis, PCA) 是一種非監督式學習的降維技術，用於找到數據中方差最大的方向（主成分），並將數據投影到這些方向上以減少特徵數量。它不直接用於分類或輸出機率。</p></div>"
                            },
                            {
                                "question_text": "決策樹 (Decision Tree) 演算法在進行節點分裂時，通常會選擇哪個特徵？",
                                "options": {
                                    "A": "隨機選擇一個特徵",
                                    "B": "選擇能夠使得分裂後子節點的「純度」最高（例如，資訊增益最大或基尼不純度最小）的特徵",
                                    "C": "選擇數值範圍最大的特徵",
                                    "D": "選擇與目標變數相關性最小的特徵"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>決策樹 (Decision Tree) 是一種監督式學習演算法，它透過一系列的決策規則（以樹狀結構表示）來進行分類或迴歸。在構建決策樹的過程中，一個關鍵步驟是如何選擇最佳的特徵來分裂當前節點，以使得分裂後的子節點盡可能「純粹」（即子節點中的樣本盡可能屬於同一類別，或迴歸值盡可能相似）。常用的分裂標準包括：</p><p>-   <strong>資訊增益 (Information Gain)</strong>：基於熵 (Entropy) 的概念，選擇能夠最大程度減少分裂後子節點熵（不確定性）的特徵，即資訊增益最大的特徵。ID3演算法使用此標準。</p><p>-   <strong>增益率 (Gain Ratio)</strong>：對資訊增益進行校正，以避免偏向於選擇具有較多取值的特徵。C4.5演算法使用此標準。</p><p>-   <strong>基尼不純度 (Gini Impurity)</strong>：衡量從數據集中隨機選擇一個樣本，然後隨機分配一個標籤，該樣本被錯誤分類的機率。選擇能夠使得分裂後子節點基尼不純度最小（即純度最高）的特徵。CART (Classification and Regression Trees) 演算法使用此標準。</p><p>這些標準的核心思想都是選擇一個能夠最好地區分不同類別樣本的特徵進行分裂。</p><p><strong>(A) 錯誤。</strong>隨機選擇特徵進行分裂是某些集成學習方法（如隨機森林中的部分隨機性）或極端隨機樹 (Extremely Randomized Trees) 的做法，但標準的決策樹演算法（如ID3, C4.5, CART）在選擇分裂特徵時是有明確的優化目標的，而不是完全隨機。</p><p><strong>(C) 錯誤。</strong>特徵的數值範圍大小本身並不直接決定其對分類的貢獻。一個數值範圍小但區分能力強的特徵可能遠比一個數值範圍大但區分能力弱的特徵更適合用於分裂。</p><p><strong>(D) 錯誤。</strong>決策樹的目標是找到與目標變數最相關、最能區分不同目標值的特徵來進行分裂，而不是選擇相關性最小的特徵。選擇相關性最小的特徵會導致分類效果很差。</p></div>"
                            },
                            {
                                "question_text": "支持向量機 (Support Vector Machine, SVM) 在進行分類 (Classification)時，其主要目標是找到什麼？",
                                "options": {
                                    "A": "一個能夠穿過最多數據點的超平面",
                                    "B": "一個能夠將不同類別數據點分開，並且使得兩邊間隔 (margin) 最大的超平面",
                                    "C": "一個能夠將所有數據點都包含在內的最小圓形",
                                    "D": "數據點在低維空間中的最佳投影"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>支持向量機 (Support Vector Machine, SVM) 是一種強大的監督式學習演算法，常用於分類和迴歸任務。在分類任務中，SVM的核心思想是找到一個能夠將不同類別的數據點分隔開來的最佳決策邊界，這個決策邊界在二維空間中是一條直線，在更高維空間中則是一個超平面 (hyperplane)。SVM的「最佳」指的是這個超平面不僅能夠正確分開不同類別的數據，而且它到最近的來自兩個不同類別的數據點（這些點被稱為支持向量，support vectors）的距離（即間隔，margin）是最大的。最大化這個間隔有助於提高模型的泛化能力，使其對新的、未見過的數據有更好的分類效果。</p><p><strong>(A) 錯誤。</strong>SVM的目標不是找到穿過最多數據點的超平面。事實上，決策超平面通常不會穿過任何數據點（除非是線性不可分的情況下使用軟間隔）。</p><p><strong>(C) 錯誤。</strong>找到一個能夠將所有數據點都包含在內的最小圓形或球體，是與支持向量數據描述 (Support Vector Data Description, SVDD) 相關的概念，主要用於異常檢測或單類別分類，而不是標準SVM分類的主要目標。</p><p><strong>(D) 錯誤。</strong>數據點在低維空間中的最佳投影通常與降維技術相關，例如主成分分析 (PCA)。雖然SVM可以與核技巧 (kernel trick) 結合，將數據映射到更高維的特徵空間以實現非線性分類，但其核心目標仍然是找到最大間隔的分類超平面，而不是尋找最佳投影。</p></div>"
                            },
                            {
                                "question_text": "K-近鄰演算法 (K-Nearest Neighbors, KNN) 如何對新的數據點進行分類 (Classification)或預測？",
                                "options": {
                                    "A": "建立一個複雜的數學模型來描述數據分佈",
                                    "B": "找出訓練集中與新數據點最接近的K個鄰居，並根據這些鄰居的標籤進行投票或平均來決定新數據點的標籤",
                                    "C": "透過梯度下降 (Gradient Descent)法 (Gradient Descent)優化一個損失函數 (Loss Function)",
                                    "D": "將數據轉換 (Transform)到一個新的特徵空間"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>K-近鄰演算法 (K-Nearest Neighbors, KNN) 是一種簡單且直觀的非參數、懶惰學習 (lazy learning) 演算法，可用於分類和迴歸任務。其核心思想是「物以類聚」。當需要對一個新的、未標記的數據點進行預測時，KNN會執行以下步驟：</p><p>1.  計算新數據點與訓練集中所有已知數據點之間的距離（常用的距離度量包括歐幾里德距離、曼哈頓距離等）。</p><p>2.  選取距離新數據點最近的K個訓練數據點（即K個最近鄰居）。K是一個由使用者指定的超參數。</p><p>3.  對於分類問題，KNN會查看這K個鄰居的類別標籤，並採用「多數投票」的原則，將新數據點預測為K個鄰居中出現次數最多的那個類別。</p><p>4.  對於迴歸問題，KNN通常會計算這K個鄰居的目標值的平均數（或加權平均數），並將其作為新數據點的預測值。</p><p><strong>(A) 錯誤。</strong>KNN是一種非參數方法，它不對數據分佈做任何假設，也不會建立一個明確的數學模型來描述數據分佈（如羅吉斯迴歸或SVM那樣）。它直接依賴於訓練樣本本身進行預測。</p><p><strong>(C) 錯誤。</strong>KNN的訓練過程非常簡單，基本上只是儲存訓練數據。它不像許多其他機器學習演算法（如神經網路、羅吉斯迴歸）那樣需要透過梯度下降法等優化演算法來最小化一個損失函數以學習模型參數。</p><p><strong>(D) 錯誤。</strong>KNN直接在原始特徵空間中計算距離並進行預測，它本身不涉及將數據轉換到一個新的特徵空間。某些演算法（如SVM中的核技巧或PCA）會進行特徵空間轉換，但這不是KNN的核心機制。</p></div>"
                            },
                            {
                                "question_text": "下列哪種演算法屬於非監督式學習 (Unsupervised Learning)，常用於將數據集自動分組成若干個相似的群體 (clusters)？",
                                "options": {
                                    "A": "隨機森林 (Random Forest)",
                                    "B": "K-均值分群 (K-Means Clustering)",
                                    "C": "梯度提升機 (Gradient Boosting Machine)",
                                    "D": "樸素貝氏分類 (Classification)器 (Naive Bayes Classifier)"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>K-均值分群 (K-Means Clustering) 是一種經典且廣泛使用的非監督式學習演算法，其主要目的是將一個未標記的數據集劃分成K個互不相交的群體（或簇，clusters）。「非監督式」意味著算法在學習過程中不需要預先定義的類別標籤。K-Means透過迭代的方式，試圖最小化每個群體內數據點到該群體中心（質心，centroid）的平方距離之和，從而使得同一群體內的數據點盡可能相似，而不同群體之間的數據點盡可能不同。K是需要使用者預先指定的群體數量。</p><p><strong>(A) 錯誤。</strong>隨機森林 (Random Forest) 是一種集成學習方法，屬於監督式學習演算法，常用於分類和迴歸問題。它透過構建多個決策樹並結合它們的預測結果來提高模型的性能和穩健性。</p><p><strong>(C) 錯誤。</strong>梯度提升機 (Gradient Boosting Machine, GBM) 也是一種集成學習方法，屬於監督式學習演算法，常用於分類和迴歸問題。它透過迭代地訓練一系列弱學習器（通常是決策樹），每個新的學習器都試圖修正前面學習器的殘差，從而逐步提升整體模型的性能。</p><p><strong>(D) 錯誤。</strong>樸素貝氏分類器 (Naive Bayes Classifier) 是一種基於貝氏定理的監督式學習演算法，常用於分類問題。它假設特徵之間在給定類別的條件下是相互獨立的（樸素假設）。</p></div>"
                            },
                            {
                                "solution": "<div><p><strong>(B) 正確。</strong>在機器學習中，「參數」與「超參數」是兩個截然不同的概念：</p><p>- <strong>參數 (Parameters)</strong>：是模型內部的變量，在訓練過程中從數據中學習並優化得到的。例如，線性迴歸中的係數、神經網絡中的權重和偏置等。這些參數直接影響模型對新數據的預測結果，是通過最小化損失函數（如梯度下降）來自動求解的。</p><p>- <strong>超參數 (Hyperparameters)</strong>：是模型外部的設置，需要在訓練前由人工設定，用於控制學習算法本身的行為。超參數不是從數據中學習得到的，而是需要通過經驗、嘗試或系統性的調參技術（如網格搜索、隨機搜索）來確定。常見的超參數包括：學習率、正則化系數、神經網絡的層數和每層神經元數量、決策樹的最大深度、集成學習中的基學習器數量等。</p><p><strong>(A) 錯誤。</strong>該選項將參數和超參數的定義顛倒了。超參數是需要人工設定的，而不是從數據中學習得到的；參數則是模型在訓練過程中學習得到的，而不是手動設定的。</p><p><strong>(C) 錯誤。</strong>參數和超參數有本質的區別：一個是模型自動學習的，另一個是人工設定的；一個直接構成模型本身，另一個控制學習過程；它們絕不能互換使用。</p><p><strong>(D) 錯誤。</strong>超參數不僅存在於深度學習模型中，幾乎所有機器學習算法都有超參數。例如，決策樹的最大深度、支持向量機的核參數、K-近鄰的K值等，都是這些傳統機器學習算法的超參數。</p></div>",
                                "question_text": "在機器學習中，「超參數 (hyperparameters)」與「參數 (parameters)」的主要區別是什麼？",
                                "options": {
                                    "A": "超參數是模型從數據中學習的變量，參數是人工手動設定的變量",
                                    "B": "參數是模型從數據中學習的變量，超參數是在訓練前人工設定的變量",
                                    "C": "兩者本質上相同，可以互換使用",
                                    "D": "超參數僅在深度學習模型中存在，傳統機器學習模型只有參數"
                                },
                                "correct_answer": "B"
                            }
                        ]
                    },
                    "L23203": {
                        "title": "深度學習原理與框架",
                        "questions": [
                            {
                                "question_text": "在類神經網路中，活化函數 (Activation Function) 的主要作用是什麼？",
                                "options": {
                                    "A": "對輸入數據進行標準化",
                                    "B": "為神經元引入非線性，使得網路能夠學習更複雜的模式",
                                    "C": "計算損失函數的值",
                                    "D": "初始化網路的權重參數"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>活化函數 (Activation Function) 是類神經網路（或深度學習模型）中神經元的一個關鍵組成部分。在神經元接收到所有輸入的加權和（再加上偏置項）之後，這個結果會被傳遞給活化函數。活化函數的主要作用是引入非線性 (non-linearity) 到網路中。如果沒有非線性活化函數，多層神經網路本質上仍然是一個線性模型，其表達能力將非常有限，無法學習和表示複雜的數據模式（例如圖像識別、自然語言理解中的複雜關係）。常見的非線性活化函數包括 Sigmoid、Tanh、ReLU (Rectified Linear Unit) 及其變體等。這些函數使得神經網路能夠逼近任意複雜的非線性函數，從而處理現實世界中的複雜問題。</p><p><strong>(A) 錯誤。</strong>對輸入數據進行標準化 (Standardization) 或歸一化 (Normalization) 是數據預處理的一個步驟，目的是將數據調整到一個合適的範圍（例如均值為0，標準差為1），以幫助模型訓練的穩定性和收斂速度。這通常在數據進入網路之前完成，而不是活化函數的主要作用。</p><p><strong>(C) 錯誤。</strong>計算損失函數 (Loss Function) 的值是在模型進行預測之後，用於衡量預測結果與真實標籤之間的差異。損失函數指導模型的優化過程，但它不是活化函數本身的功能。</p><p><strong>(D) 錯誤。</strong>初始化網路的權重參數是模型訓練開始前的一個重要步驟，目的是為權重賦予初始值。權重初始化的好壞會影響模型的訓練效果，但這與活化函數的作用不同。活化函數是在前向傳播過程中對神經元的輸出進行非線性變換。</p></div>"
                            },
                            {
                                "question_text": "反向傳播演算法 (Backpropagation) 在深度學習模型訓練中的核心功能是什麼？",
                                "options": {
                                    "A": "對輸入圖像進行特徵提取",
                                    "B": "高效地計算損失函數對於網路中所有權重參數的梯度，以便進行權重更新",
                                    "C": "隨機初始化神經網路的權重",
                                    "D": "決定神經網路的層數與每層的神經元數量"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>反向傳播演算法 (Backpropagation) 是訓練多層神經網路（深度學習模型）的核心演算法。在模型進行一次前向傳播 (forward pass) 並計算出預測值與損失函數值之後，反向傳播演算法會從輸出層開始，逐層向後計算損失函數對於網路中每一個權重 (weight) 和偏置 (bias) 參數的梯度（即偏導數）。這些梯度指明了每個參數應該如何調整才能使損失函數減小。一旦計算出所有梯度，就可以使用梯度下降法（或其變體，如SGD, Adam等）來更新網路的參數。反向傳播的關鍵在於它利用了微積分中的鏈式法則 (chain rule)，能夠高效地計算這些梯度，即使對於非常深和複雜的網路也是如此。</p><p><strong>(A) 錯誤。</strong>對輸入圖像進行特徵提取是卷積神經網路 (CNN) 等模型在前向傳播過程中的一部分功能，例如卷積層和池化層會自動學習和提取圖像特徵。反向傳播是發生在特徵提取和預測之後，用於計算梯度以更新模型參數的過程。</p><p><strong>(C) 錯誤。</strong>隨機初始化神經網路的權重是在模型訓練開始之前進行的一個重要步驟，目的是打破對稱性並幫助模型開始學習。反向傳播是在權重初始化之後，用於在訓練過程中調整這些權重。</p><p><strong>(D) 錯誤。</strong>決定神經網路的層數與每層的神經元數量是模型架構設計的一部分，通常由人工設計或透過神經架構搜索 (Neural Architecture Search, NAS) 等技術來確定。反向傳播演算法本身不決定網路的架構，而是在給定架構下訓練模型參數。</p></div>"
                            },
                            {
                                "question_text": "卷積神經網路 (Convolutional Neural Network, CNN) 特別擅長處理哪種類型的數據？其核心組件通常包含哪些層？",
                                "options": {
                                    "A": "時間序列數據；循環層、注意力層",
                                    "B": "表格數據；全連接層、決策樹層",
                                    "C": "圖像或網格狀數據；卷積層、池化層、全連接層",
                                    "D": "文字數據；嵌入層、LSTM層"
                                },
                                "correct_answer": "C",
                                "solution": "<div><p><strong>(C) 正確。</strong>卷積神經網路 (Convolutional Neural Network, CNN 或 ConvNet) 是一種特殊設計的深度學習模型，因其在處理具有網格狀拓撲結構的數據方面表現出色而聞名，其中最典型的應用就是圖像處理和電腦視覺任務。CNN的核心思想是利用卷積運算來自動學習和提取數據中的局部空間層次特徵。其核心組件通常包括：</p><p>- <strong>卷積層 (Convolutional Layer)</strong>：透過可學習的濾波器（卷積核）對輸入數據進行卷積操作，以提取局部特徵，如邊緣、角點、紋理等。</p><p>- <strong>池化層 (Pooling Layer)</strong>：通常位於卷積層之後，用於降低特徵圖的空間維度（下採樣），減少計算量，並增強模型的平移不變性。常見的池化操作有最大池化 (Max Pooling) 和平均池化 (Average Pooling)。</p><p>- <strong>全連接層 (Fully Connected Layer)</strong>：在經過多個卷積層和池化層提取特徵後，通常會將特徵圖展平並連接到一個或多個全連接層，用於進行最終的分類或迴歸預測（類似於傳統的多層感知器）。</p><p><strong>(A) 錯誤。</strong>時間序列數據（如語音、股票價格）通常更適合使用循環神經網路 (RNN) 或其變體（如LSTM, GRU）以及注意力機制來處理，因為這些模型能夠捕捉序列中的時間依賴性。雖然CNN有時也用於時間序列（例如一維CNN），但其最擅長的領域是圖像等網格數據。</p><p><strong>(B) 錯誤。</strong>表格數據（結構化數據）通常使用傳統的機器學習演算法（如決策樹、隨機森林、梯度提升樹、支持向量機）或簡單的全連接神經網路（多層感知器）來處理。決策樹層不是標準CNN的組件。</p><p><strong>(D) 錯誤。</strong>文字數據（自然語言處理任務）傳統上常使用RNN、LSTM、GRU等模型，近年來Transformer模型（基於自注意力機制）取得了巨大成功。嵌入層 (Embedding Layer) 用於將文字轉換為向量表示，是處理文字數據的常見初始步驟，但CNN本身的核心組件不直接包含LSTM層。</p></div>"
                            },
                            {
                                "question_text": "循環神經網路 (Recurrent Neural Network, RNN) 為何適合處理序列數據（如文字、語音）？",
                                "options": {
                                    "A": "因為其網路結構中不包含任何循環連接",
                                    "B": "因為其隱藏層的狀態可以在不同時間步之間傳遞，從而捕捉序列中的時間依賴性",
                                    "C": "因為它只能處理固定長度的輸入序列",
                                    "D": "因為它不需要進行反向傳播訓練"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>循環神經網路 (Recurrent Neural Network, RNN) 的核心特點是其網路結構中包含循環連接（或稱為反饋迴路）。這使得網路的隱藏層狀態不僅依賴於當前時間步的輸入，還依賴於前一個時間步的隱藏層狀態。這種「記憶」機制允許信息在序列的不同時間步之間傳遞和保持，從而使RNN能夠有效地捕捉和學習序列數據（如文字、語音、時間序列數據等）中的時間依賴關係和上下文信息。例如，在處理一個句子時，RNN可以利用前面詞語的信息來理解當前詞語的含義。</p><p><strong>(A) 錯誤。</strong>這與RNN的定義相反。RNN的關鍵就在於其結構中存在循環連接，使得信息可以在時間步之間流動。</p><p><strong>(C) 錯誤。</strong>雖然傳統的簡單RNN在處理非常長的序列時可能會遇到梯度消失或梯度爆炸的問題，並且在實際應用中通常需要將序列截斷或填充到固定長度進行批次處理，但RNN的設計理念是能夠處理可變長度的序列。更高級的RNN變體，如長短期記憶網路 (LSTM) 和門控循環單元 (GRU)，被設計用來更好地處理長序列依賴問題。</p><p><strong>(D) 錯誤。</strong>RNN（包括其變體LSTM、GRU等）通常也是透過反向傳播演算法進行訓練的，不過是其適用於序列數據的變體，稱為隨時間反向傳播 (Backpropagation Through Time, BPTT)。BPTT將循環網路在時間上展開成一個深層的前饋網路，然後應用標準的反向傳播來計算梯度並更新權重。</p></div>"
                            },
                            {
                                "question_text": "Transformer 模型近年來在自然語言處理領域取得了巨大成功，其核心機制是什麼？",
                                "options": {
                                    "A": "卷積運算 (Convolution)",
                                    "B": "循環連接 (Recurrent Connection)",
                                    "C": "注意力機制 (Attention Mechanism)，特別是自注意力機制 (Self-Attention)",
                                    "D": "K-均值分群 (K-Means Clustering)"
                                },
                                "correct_answer": "C",
                                "solution": "<div><p><strong>(C) 正確。</strong>Transformer 模型（由 Vaswani 等人在論文 \"Attention Is All You Need\" 中提出）已經成為自然語言處理 (NLP) 領域的革命性架構，並在機器翻譯、文本生成、問答系統等眾多任務上取得了最先進的成果。其核心創新和成功的關鍵在於完全拋棄了傳統的循環連接 (RNN) 和卷積運算 (CNN) 作為主要的序列處理單元，而是完全依賴於注意力機制 (Attention Mechanism)，特別是其中的自注意力機制 (Self-Attention Mechanism)。自注意力機制允許模型在處理序列中的每個元素（例如一個詞）時，能夠同時關注到序列中所有其他元素，並根據它們之間的相關性來計算該元素的表示，從而有效地捕捉長距離依賴關係，並且能夠並行處理序列中的所有元素，大大提高了訓練效率。</p><p><strong>(A) 錯誤。</strong>雖然卷積運算 (Convolution) 在某些NLP模型（如文本CNN）中被用於提取局部特徵，但Transformer模型的核心並不是卷積運算。Transformer的設計初衷之一就是擺脫對卷積和循環的依賴。</p><p><strong>(B) 錯誤。</strong>循環連接 (Recurrent Connection) 是RNN及其變體（如LSTM, GRU）的核心特徵，用於按順序處理序列數據並捕捉時間依賴性。Transformer模型的一個主要突破就是不再使用循環連接，從而克服了RNN在處理長序列時的梯度消失/爆炸問題以及難以並行計算的缺點。</p><p><strong>(D) 錯誤。</strong>K-均值分群 (K-Means Clustering) 是一種非監督式學習演算法，用於將數據自動分組，與Transformer這種用於序列建模的監督式（或自監督式）深度學習架構的核心機制無關。</p></div>"
                            }
                        ]
                    },
                    "L23204": {
                        "title": "特徵工程與特徵選擇",
                        "questions": [
                            {
                                "question_text": "特徵工程 (Feature Engineering) 在機器學習中的重要性在於什麼？",
                                "options": {
                                    "A": "創建對模型效能有正面影響的新特徵，或轉換 (Transform)現有特徵，以更好地反映數據中的潛在規律",
                                    "B": "增加模型的複雜度以避免欠擬合 (Underfitting)",
                                    "C": "減少計算資源的使用",
                                    "D": "處理邏輯歸納的推論"
                                },
                                "correct_answer": "A",
                                "solution": "<div><p><strong>(A) 正確。</strong>特徵工程 (Feature Engineering) 是機器學習中非常重要的一個環節，它直接影響模型的性能。特徵工程的主要目的是創建對模型效能有正面影響的新特徵，或轉換現有特徵，以更好地表示和捕捉數據中的潛在規律和模式。好的特徵工程可以：</p><p>-   <strong>使模型更準確</strong>：如果特徵能夠很好地表達目標變量的相關信息，模型更容易學習到真實的模式。</p><p>-   <strong>提高訓練效率</strong>：好的特徵可以使模型更容易、更快速地學習到有用的模式。</p><p>-   <strong>增強模型的泛化能力</strong>：合適的特徵可以幫助模型更好地泛化到未見過的數據。</p><p>特徵工程的方法包括特徵選擇、特徵提取、特徵轉換（如歸一化、標準化）、特徵創建（如多項式特徵、交互特徵）等。</p><p><strong>(B) 錯誤。</strong>增加模型的複雜度可能有助於避免欠擬合，但這不是特徵工程的主要目的。特徵工程的重點是改善特徵以更好地表示數據中的關係，而非直接增加模型的複雜度。</p><p><strong>(C) 錯誤。</strong>良好的特徵工程可能會減少所需的計算資源（例如，通過特徵選擇減少特徵數量），但這通常是一個副作用，而非特徵工程的主要目的。特徵工程的主要目的是提高模型效能。</p><p><strong>(D) 錯誤。</strong>處理邏輯歸納的推論通常是推理系統或專家系統的任務，而非特徵工程的主要目的。特徵工程是將數據轉換為更適合機器學習模型的形式，以提高學習效率和模型性能。</p></div>"
                            },
                            {
                                "question_text": "特徵選擇 (Feature Selection) 的主要目的是什麼？",
                                "options": {
                                    "A": "增加訓練數據的數量",
                                    "B": "降低模型的泛化能力",
                                    "C": "選擇最相關的特徵以提升模型性能，降低複雜度，並減少過擬合 (Overfitting)",
                                    "D": "提高模型對噪聲 (noise)的敏感性"
                                },
                                "correct_answer": "C",
                                "solution": "<div><p><strong>(C) 正確。</strong>特徵選擇 (Feature Selection) 是指從原始特徵集中選擇出一個子集，以降低特徵維度並提升模型性能。其主要目的包括：</p><p>1.  <strong>選擇最相關的特徵</strong>：識別並保留那些與目標變量最相關、對預測結果最有影響力的特徵，同時排除不相關或冗餘的特徵。</p><p>2.  <strong>提升模型性能</strong>：通過去除噪聲特徵，模型可以集中學習真正相關的模式，從而提高預測準確性。</p><p>3.  <strong>降低模型複雜度</strong>：減少特徵數量可以簡化模型，降低計算復雜度，加快訓練和預測速度。</p><p>4.  <strong>減少過擬合風險</strong>：較少的特徵降低了模型的自由度，使其不太可能捕捉到訓練數據中的噪聲，從而提高模型在未見數據上的泛化能力。</p><p>5.  <strong>改善模型可解釋性</strong>：較少的特徵通常使模型更易於理解和解釋，尤其對於如線性模型或決策樹這樣的簡單模型。</p><p><strong>(A) 錯誤。</strong>特徵選擇不會增加訓練數據的數量，它是減少特徵數量的過程，而非增加樣本數量。增加訓練數據的方法包括收集更多數據、使用數據增強技術等。</p><p><strong>(B) 錯誤。</strong>特徵選擇的目的之一是通過去除不相關或冗餘特徵來減少過擬合，從而提高模型的泛化能力，而非降低泛化能力。</p><p><strong>(D) 錯誤。</strong>特徵選擇旨在選擇最相關、最有信息量的特徵，同時排除噪聲特徵，從而使模型對噪聲不那麼敏感，而非提高模型對噪聲的敏感性。</p></div>"
                            },
                            {
                                "question_text": "下列哪個不屬於特徵選擇 (Feature Selection)的常見方法？",
                                "options": {
                                    "A": "過濾法 (Filter Methods)",
                                    "B": "包裝法 (Wrapper Methods)",
                                    "C": "嵌入法 (Embedded Methods)",
                                    "D": "轉換 (Transform)維度法 (Dimension Transform Methods)"
                                },
                                "correct_answer": "D",
                                "solution": "<div><p><strong>(D) 正確。</strong>「轉換維度法 (Dimension Transform Methods)」不是特徵選擇的常見方法。實際上，這是一個牽強或混淆的術語，可能是指特徵提取或降維技術（如主成分分析 PCA 或 t-SNE），而非傳統意義上的特徵選擇。特徵選擇是指從原始特徵集中選擇出一個子集，保留原特徵而不創建新特徵；而降維（如 PCA）則是將高維特徵轉換為低維特徵，通常會創建新的特徵表示。特徵選擇的三種主要方法是過濾法、包裝法和嵌入法。</p><p><strong>(A) 錯誤。</strong>過濾法 (Filter Methods) 是一種特徵選擇方法，它基於特徵與目標變量之間的統計指標（如相關係數、互信息、卡方檢驗等）來評估每個特徵的重要性，然後選擇排名靠前的特徵。這類方法獨立於後續使用的學習算法，計算效率高，但可能無法捕捉到特徵之間的相互作用。</p><p><strong>(B) 錯誤。</strong>包裝法 (Wrapper Methods) 是一種特徵選擇方法，它使用目標機器學習算法的表現來評估特徵子集的好壞。它通過訓練模型來評估不同特徵子集的性能（如通過交叉驗證），然後選擇能使模型性能最優的特徵子集。常見的包裝法包括遞歸特徵消除 (Recursive Feature Elimination)、前向選擇 (Forward Selection)、後向消除 (Backward Elimination) 等。包裝法能夠捕捉特徵間的相互作用，但計算成本較高。</p><p><strong>(C) 錯誤。</strong>嵌入法 (Embedded Methods) 是一種特徵選擇方法，它將特徵選擇過程嵌入到模型訓練過程中。這些方法在模型學習的同時進行特徵選擇，通常是通過為模型參數增加懲罰項（如 L1 正則化，即 Lasso）來實現。嵌入法結合了過濾法的效率和包裝法的有效性，是一種較為平衡的方法。常見的嵌入法包括 Lasso 回歸、具有特徵選擇能力的樹模型等。</p></div>"
                            },
                            {
                                "question_text": "在數據預處理中，標準化 (Standardization) 和歸一化 (Normalization) 的主要區別是什麼？",
                                "options": {
                                    "A": "標準化 (Standardization)將數據轉換 (Transform)為標準常態分佈，歸一化 (Normalization)將數據縮放到特定範圍（通常是0到1之間）",
                                    "B": "兩者完全相同，是可互換的術語",
                                    "C": "標準化 (Standardization)用於分類 (Classification)任務，歸一化 (Normalization)用於迴歸 (Regression)任務",
                                    "D": "標準化 (Standardization)適用於離散型特徵，歸一化 (Normalization)適用於連續型特徵"
                                },
                                "correct_answer": "A",
                                "solution": "<div><p><strong>(A) 正確。</strong>標準化 (Standardization) 和歸一化 (Normalization) 是兩種不同的數據預處理技術，它們的主要區別在於轉換後數據的性質：</p><p>-   <strong>標準化 (Standardization)</strong>：又稱為 Z-score 正規化，它將數據轉換為均值為0、標準差為1的標準常態分佈。標準化後的數據沒有確定的範圍，理論上可以取任何值。標準化公式為：</p><p>    Z = (X - μ) / σ</p><p>    其中，X 是原始數據，μ 是數據的均值，σ 是數據的標準差。</p><p>-   <strong>歸一化 (Normalization)</strong>：也稱為 Min-Max 縮放，它將數據線性轉換到一個特定的範圍內，通常是 [0, 1] 或 [-1, 1]。歸一化公式為：</p><p>    X_norm = (X - X_min) / (X_max - X_min)</p><p>    其中，X 是原始數據，X_min 和 X_max 分別是數據的最小值和最大值。</p><p><strong>(B) 錯誤。</strong>標準化和歸一化不是完全相同的術語，它們代表了不同的數據轉換技術，如上所述。</p><p><strong>(C) 錯誤。</strong>標準化和歸一化的適用情況與任務類型（分類或迴歸）沒有直接對應關係。選擇哪種方法通常取決於數據的性質、算法的需求以及特定的問題背景。例如，梯度下降通常對歸一化後的數據效果更好；主成分分析 (PCA) 通常需要標準化的數據；對於基於距離的算法如 K-近鄰或 K-均值，歸一化可以確保不同特徵的距離計算公平。</p><p><strong>(D) 錯誤。</strong>標準化和歸一化都主要適用於連續型特徵。對於離散型特徵，特別是無序的類別型特徵，通常使用其他方法如獨熱編碼 (One-Hot Encoding) 來處理。標準化和歸一化不是基於特徵的數據類型（離散或連續）來區分的。</p></div>"
                            },
                            {
                                "question_text": "下列哪個不屬於有效的特徵工程 (Feature Engineering)技術？",
                                "options": {
                                    "A": "主成分分析 (Principal Component Analysis, PCA)",
                                    "B": "獨熱編碼 (One-Hot Encoding)",
                                    "C": "多項式特徵創建 (Polynomial Feature Creation)",
                                    "D": "降低訓練數據量 (Reducing Training Data)"
                                },
                                "correct_answer": "D",
                                "solution": "<div><p><strong>(D) 正確。</strong>「降低訓練數據量」不是特徵工程的一種技術，相反，它可能損害模型的性能。更多的訓練數據通常有助於模型學習更準確的模式和關係，而減少數據量可能導致模型欠擬合或無法捕捉足夠的數據變異性。特徵工程是關於如何轉換或選擇特徵以提高模型性能，而不是減少訓練數據量。</p><p><strong>(A) 錯誤。</strong>主成分分析 (PCA) 是一種特徵提取和降維技術，它將原始特徵轉換為一組新的、互相正交的特徵（稱為主成分），並按其解釋的數據變異量排序。通過選擇前k個主成分，可以在保留大部分數據信息的前提下降低特徵維度，這確實是特徵工程的一種有效技術。</p><p><strong>(B) 錯誤。</strong>獨熱編碼 (One-Hot Encoding) 是一種處理類別型特徵的技術，它將一個類別型特徵轉換為多個二元特徵。例如，如果一個特徵有三個可能的類別值 (A, B, C)，獨熱編碼會將其轉換為三個特徵 (Is_A, Is_B, Is_C)，每個特徵的值是 0 或 1。這是特徵工程中處理類別型數據的標準方法。</p><p><strong>(C) 錯誤。</strong>多項式特徵創建 (Polynomial Feature Creation) 是一種特徵工程技術，它通過創建原始特徵的多項式組合（如平方項、交叉項等）來捕捉特徵之間的非線性關係。例如，如果原始特徵是 x1 和 x2，多項式特徵可能包括 x1^2, x1*x2, x2^2 等。這種技術可以增強模型捕捉複雜關係的能力，尤其是對於線性模型而言。</p></div>"
                            }
                        ]
                    }
                }
            },
            "L233": {
                "title": "機器學習建模與參數調校",
                "sub_sections": {
                    "L23301": {
                        "title": "數據準備與特徵工程",
                        "questions": [
                            {
                                "question_text": "在機器學習中，特徵工程 (Feature Engineering) 為何重要？",
                                "options": {
                                    "A": "它通常是模型訓練中最耗時但最不重要的環節",
                                    "B": "透過創建更能反映數據內在規律的特徵，可以顯著提升模型的性能",
                                    "C": "好的特徵工程可以完全取代模型選擇的過程",
                                    "D": "特徵工程僅適用於非監督學習"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>特徵工程 (Feature Engineering) 是指利用領域知識和數據分析技能，從原始數據中提取、轉換或創建新的特徵，以供機器學習模型使用的過程。它是機器學習流程中至關重要的一環，因為模型的性能上限很大程度上取決於輸入特徵的質量。好的特徵能夠更直接、更有效地揭示數據中與預測目標相關的內在規律和模式，從而使得即使是簡單的模型也能取得良好的性能。所謂「數據和特徵決定了機器學習的上限，而模型和演算法只是在逼近這個上限」。</p><p><strong>(A) 錯誤。</strong>特徵工程雖然可能非常耗時，但它絕不是最不重要的環節，反而往往是對模型最終性能影響最大的環節之一。有經驗的數據科學家通常會花費大量時間在特徵工程上。</p><p><strong>(C) 錯誤。</strong>雖然好的特徵工程可以極大地提升模型性能，甚至使得簡單模型也能表現優異，但它不能完全取代模型選擇的過程。不同的模型適用於不同類型的數據和問題，即使有好的特徵，選擇合適的模型仍然是必要的。特徵工程和模型選擇是相輔相成的。</p><p><strong>(D) 錯誤。</strong>特徵工程不僅適用於非監督學習（例如，為分群演算法創建更好的特徵），也同樣重要甚至更常應用於監督式學習（例如，為分類或迴歸模型創建預測能力更強的特徵）。無論是哪種類型的學習任務，好的特徵都是提升模型性能的關鍵。</p></div>"
                            },
                            {
                                "question_text": "處理類別型特徵 (Categorical Features) 時，獨熱編碼 (One-Hot Encoding) 的主要作用是什麼？",
                                "options": {
                                    "A": "將類別特徵轉換為單一的數值，保留其大小順序關係",
                                    "B": "將具有N個可能類別的特徵轉換為N個二元特徵，避免引入不存在的順序關係",
                                    "C": "降低特徵的維度",
                                    "D": "填充缺失值"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>類別型特徵 (Categorical Features) 是指其值來自一個有限的、離散的集合，例如「顏色」（紅、綠、藍）或「城市」（台北、台中、高雄）。許多機器學習演算法無法直接處理這些非數值的類別標籤。獨熱編碼 (One-Hot Encoding) 是一種常用的將類別型特徵轉換為數值型特徵的方法。對於一個具有N個不同可能類別的原始特徵，獨熱編碼會創建N個新的二元（0或1）特徵。對於每個樣本，在其原始類別對應的新特徵上取值為1，而在其他N-1個新特徵上取值為0。這樣做的好處是，它將類別型數據轉換為演算法可以處理的數值格式，並且避免了直接將類別賦予數值（如紅=1, 綠=2, 藍=3）可能引入的不存在的順序關係（例如，藍色並不一定「大於」綠色）。</p><p><strong>(A) 錯誤。</strong>將類別特徵轉換為單一的數值並保留其大小順序關係，描述的是序數編碼 (Ordinal Encoding) 或標籤編碼 (Label Encoding) 的一種情況，適用於本身具有順序性的類別特徵（例如「學歷」：高中、大學、碩士）。對於沒有內在順序的名目型特徵 (Nominal Features)，使用這種編碼會引入誤導性的順序信息。獨熱編碼正是為了解決這個問題。</p><p><strong>(C) 錯誤。</strong>獨熱編碼通常會增加特徵的維度，而不是降低。如果一個類別特徵有N個可能的類別，獨熱編碼後會產生N個新的二元特徵。當N很大時，這可能導致維度災難 (curse of dimensionality)。降維是使用PCA等技術來減少特徵數量。</p><p><strong>(D) 錯誤。</strong>填充缺失值 (Missing Values) 是數據預處理中處理數據不完整性的一個步驟，例如使用平均數、中位數填充，或使用模型預測填充。獨熱編碼是用於轉換已有的類別型特徵的表示方式，而不是處理缺失值。</p></div>"
                            },
                            {
                                "question_text": "當訓練數據集中不同類別的樣本數量差異巨大時（例如，欺詐檢測中欺詐樣本遠少於正常樣本），這種情況稱為什麼？應如何處理？",
                                "options": {
                                    "A": "過擬合；增加模型複雜度",
                                    "B": "數據稀疏；使用降維技術",
                                    "C": "數據不平衡 (Imbalanced Data)；可採用過採樣、欠採樣、代價敏感學習等方法",
                                    "D": "特徵共線性；移除相關特徵"
                                },
                                "correct_answer": "C",
                                "solution": "<div><p><strong>(C) 正確。</strong>當訓練數據集中不同類別的樣本數量差異巨大時，這種情況被稱為數據不平衡 (Imbalanced Data) 或類別不平衡 (Class Imbalance)。例如，在金融欺詐檢測中，欺詐交易的樣本數量通常遠遠少於正常交易的樣本數量；在醫療診斷中，患有罕見疾病的病患樣本也遠少於健康個體的樣本。數據不平衡會對許多標準的機器學習演算法產生負面影響，因為這些演算法通常假設類別分佈是相對均衡的，它們可能會過度關注多數類別而忽略少數類別，導致對少數類別的預測性能很差。處理數據不平衡問題的常見方法包括：</p><p>- <strong>過採樣 (Oversampling)</strong>：增加少數類別的樣本數量，例如隨機複製少數類樣本，或使用SMOTE (Synthetic Minority Over-sampling Technique) 等方法生成新的合成少數類樣本。</p><p>- <strong>欠採樣 (Undersampling)</strong>：減少多數類別的樣本數量，例如隨機移除部分多數類樣本，或使用一些更智能的欠採樣方法。</p><p>- <strong>代價敏感學習 (Cost-Sensitive Learning)</strong>：在模型的損失函數中為不同類別的錯分類賦予不同的代價（例如，將錯分類少數類別的代價設得更高），使得模型更關注少數類別的正確分類。</p><p>- <strong>集成方法</strong>：使用專門設計用於處理不平衡數據的集成學習方法，如EasyEnsemble, BalanceCascade等。</p><p>- <strong>選擇合適的評估指標</strong>：避免僅使用準確率 (Accuracy) 作為評估指標，因為在高不平衡情況下它可能具有誤導性。應考慮使用如精確率 (Precision)、召回率 (Recall)、F1分數 (F1-score)、AUC-ROC、AUC-PR等對不平衡更敏感的指標。</p><p><strong>(A) 錯誤。</strong>過擬合 (Overfitting) 是指模型在訓練集上表現好但在測試集上表現差的現象。雖然數據不平衡可能間接導致模型對多數類的過擬合，但問題本身描述的是數據不平衡。增加模型複雜度通常會加劇過擬合，而不是解決數據不平衡的方法。</p><p><strong>(B) 錯誤。</strong>數據稀疏 (Data Sparsity) 通常指數據集中大部分特徵值為零或缺失的情況，常見於高維數據（如文本數據的詞袋表示）。雖然數據稀疏和數據不平衡都可能給模型訓練帶來挑戰，但它們是不同的問題。使用降維技術是處理數據稀疏或高維數據的一種方法，但不直接解決類別不平衡。</p><p><strong>(D) 錯誤。</strong>特徵共線性 (Multicollinearity) 是指模型中的自變量（特徵）之間存在高度相關關係。這會影響迴歸模型參數估計的穩定性和解釋性。移除相關特徵是處理共線性的一種方法，但與數據不平衡問題無關。</p></div>"
                            },
                            {
                                "question_text": "特徵縮放 (Feature Scaling)，例如標準化 (Standardization) 或歸一化 (Normalization)，在某些機器學習演算法中為何是必要的？",
                                "options": {
                                    "A": "為了增加特徵的數量",
                                    "B": "確保所有特徵都在相同的數值範圍內，避免某些特徵因數值較大而主導模型的學習過程，有助於梯度下降等演算法的收斂",
                                    "C": "為了將數值特徵轉換為類別特徵",
                                    "D": "特徵縮放只會降低模型性能"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>特徵縮放 (Feature Scaling) 是數據預處理中一個非常重要的步驟，尤其對於那些依賴於特徵數值大小或距離計算的機器學習演算法而言。常見的特徵縮放方法包括：</p><p>- <strong>標準化 (Standardization)</strong>：將特徵數據轉換為均值為0，標準差為1的分佈（也稱為Z-score normalization）。計算公式為：(x - μ) / σ，其中μ是特徵的均值，σ是特徵的標準差。</p><p>- <strong>歸一化 (Normalization)</strong>：通常指將特徵數據縮放到一個固定的範圍，例如[0, 1]或[-1, 1]。最常見的歸一化方法是最小-最大歸一化 (Min-Max Scaling)，計算公式為：(x - min) / (max - min)。</p><p>特徵縮放的主要原因和好處包括：</p><p>1. <strong>避免數值較大的特徵主導模型學習</strong>：如果不同特徵的數值範圍差異很大（例如，一個特徵的範圍是0-1，另一個是0-10000），那麼在使用梯度下降等優化演算法時，數值較大的特徵可能會在損失函數的計算和梯度更新中佔據主導地位，使得模型對這些特徵更敏感，而忽略了數值較小的特徵。特徵縮放可以將所有特徵置於可比較的尺度上。</p><p>2. <strong>有助於梯度下降等演算法的收斂</strong>：當特徵尺度不一時，損失函數的等高線圖可能會呈現扁長的橢圓形，導致梯度下降的收斂路徑曲折且緩慢。特徵縮放可以使等高線圖更接近圓形，從而加速梯度下降的收斂速度並提高其穩定性。</p><p>3. <strong>適用於基於距離的演算法</strong>：像K-近鄰 (KNN)、支持向量機 (SVM，尤其是使用RBF等核函數時)、K-均值分群 (K-Means) 等依賴於計算數據點之間距離的演算法，如果特徵尺度不一，數值範圍大的特徵會在距離計算中佔據主導，影響演算法的性能。特徵縮放可以確保所有特徵對距離的貢獻是公平的。</p><p><strong>(A) 錯誤。</strong>特徵縮放是改變現有特徵的數值範圍，而不是增加特徵的數量。增加特徵數量通常是透過特徵創建 (Feature Creation) 或特徵選擇的逆過程來實現的。</p><p><strong>(C) 錯誤。</strong>將數值特徵轉換為類別特徵是離散化 (Discretization) 或分箱 (Binning) 的過程，例如將年齡（數值）轉換為年齡段（類別）。特徵縮放處理的是數值特徵的尺度，而不改變其類型。</p><p><strong>(D) 錯誤。</strong>對於許多演算法而言，適當的特徵縮放不僅不會降低模型性能，反而通常是提升模型性能和訓練效率的關鍵步驟。當然，對於某些不受特徵尺度影響的演算法（如決策樹、隨機森林等基於樹的模型），特徵縮放可能不是必需的，但也不會顯著降低性能。</p></div>"
                            },
                            {
                                "question_text": "下列何者不是常見的處理數據中缺失值 (Missing Values) 的方法？",
                                "options": {
                                    "A": "刪除包含缺失值的樣本或特徵",
                                    "B": "使用平均數、中位數或眾數填充缺失值",
                                    "C": "使用機器學習模型預測並填充缺失值",
                                    "D": "將所有缺失值都視為一個新的獨立類別，而不進行任何填充"
                                },
                                "correct_answer": "D",
                                "solution": "<div><p><strong>(D) 正確。</strong>雖然將缺失值視為一個新的獨立類別（例如，對於一個類別型特徵，如果原來有A, B兩類，可以將缺失值視為第三類C）在某些特定情況下可能是一種處理方式，尤其是在缺失本身可能帶有某種信息時（Missing Not At Random, MNAR），但它並不像其他選項那樣被廣泛認為是「常見的」普適性處理缺失值的方法。更重要的是，題目問的是「不是常見的」方法，而其他三個選項都是非常標準和常見的處理策略。</p><p>- 將所有缺失值都視為一個新的獨立類別，而不進行任何填充：這種方法對於數值型特徵通常不適用（除非先將數值特徵離散化）。對於類別型特徵，雖然可行，但如果缺失值數量很少，可能會產生一個樣本量極小的新類別，對模型學習不利。如果缺失值是隨機發生的（Missing Completely At Random, MCAR 或 Missing At Random, MAR），將其視為一個獨立類別可能引入偏差。</p><p>以下是常見的處理缺失值的方法：</p><p><strong>(A) 錯誤，這是常見方法。</strong>刪除包含缺失值的樣本（行刪除）或特徵（列刪除）是一種簡單直接的方法。如果某個樣本的缺失值過多，或者某個特徵的缺失比例過高且該特徵不重要，可以考慮刪除。但這種方法可能會導致有價值信息的損失，尤其是在數據量較少的情況下。</p><p><strong>(B) 錯誤，這是常見方法。</strong>對於數值型特徵，可以使用該特徵的平均數 (mean)、中位數 (median) 或眾數 (mode) 來填充缺失值。中位數對異常值更穩健。對於類別型特徵，可以使用眾數來填充。</p><p><strong>(C) 錯誤，這是常見方法。</strong>可以使用機器學習模型（如K-近鄰、迴歸模型、決策樹等）來預測並填充缺失值。例如，可以將帶有缺失值的特徵作為目標變數，使用其他特徵作為預測變數來訓練一個模型，然後用模型的預測結果填充缺失值。這種方法通常比簡單的均值/中位數填充更準確，但計算成本也更高。</p></div>"
                            }
                        ]
                    },
                    "L23302": {
                        "title": "模型選擇與架構設計",
                        "questions": [
                            {
                                "question_text": "在選擇機器學習模型時，應主要考慮哪些因素？",
                                "options": {
                                    "A": "僅考慮模型的複雜度和新穎性",
                                    "B": "問題的類型（分類、迴歸、分群等）、數據的特性（大小、維度、類型）、計算資源、模型的可解釋性需求等",
                                    "C": "選擇訓練時間最短的模型",
                                    "D": "選擇程式碼最少的模型"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>選擇合適的機器學習模型是一個綜合考量的過程，沒有一體適用的最佳模型（「沒有免費的午餐」定理 No Free Lunch Theorem）。主要應考慮的因素包括：</p><p>- <strong>問題的類型</strong>：是分類問題（預測離散類別）、迴歸問題（預測連續數值）、分群問題（無監督地將數據分組）、降維問題，還是其他類型的問題？不同類型的問題有其對應的適用演算法。</p><p>- <strong>數據的特性</strong>：</p><p>  - <strong>數據大小（樣本量）</strong>：樣本量的大小會影響模型的選擇。小數據集可能更適合簡單模型以避免過擬合，而大數據集則可以支持更複雜的模型。</p><p>  - <strong>數據維度（特徵數量）</strong>：高維數據可能需要降維處理或使用對高維數據不敏感的模型。</p><p>  - <strong>數據類型</strong>：數據是數值型、類別型、文本、圖像、時間序列等？不同類型的數據有其專門的處理方法和模型。</p><p>  - <strong>數據質量</strong>：數據中是否存在缺失值、異常值、噪聲等？這些都會影響模型的選擇和預處理策略。</p><p>  - <strong>數據的線性可分性</strong>：如果數據是線性可分的，簡單的線性模型可能就足夠了；如果數據具有複雜的非線性結構，則可能需要非線性模型（如核SVM、神經網路）。</p><p>- <strong>計算資源</strong>：可用的CPU、GPU、記憶體等計算資源會限制模型的複雜度和訓練時間。某些複雜模型（如大型深度學習模型）需要大量的計算資源。</p><p>- <strong>模型的可解釋性需求</strong>：在某些應用場景（如金融風控、醫療診斷），模型的可解釋性非常重要，需要能夠理解模型是如何做出決策的。在這種情況下，可能會選擇本身具有較好可解釋性的模型（如決策樹、線性迴歸），或者使用XAI技術來解釋複雜模型。</p><p>- <strong>訓練時間與預測時間要求</strong>：某些應用對模型的訓練速度或預測速度有嚴格要求。</p><p>- <strong>模型的性能指標</strong>：根據具體問題，選擇合適的評估指標（如準確率、精確率、召回率、F1分數、MSE等）來衡量和比較不同模型的性能。</p><p><strong>(A) 錯誤。</strong>僅僅追求模型的複雜度和新穎性是不可取的。複雜模型更容易過擬合，且訓練和維護成本更高。新穎的模型可能尚未經過充分驗證。應根據實際問題和數據選擇最合適的模型，而不是盲目追求複雜或新潮。</p><p><strong>(C) 錯誤。</strong>雖然訓練時間是一個需要考慮的因素，但選擇訓練時間最短的模型並不一定能得到最佳的性能。有時，訓練時間較長但性能更好的模型可能是更優的選擇，需要在效率和效果之間進行權衡。</p><p><strong>(D) 錯誤。</strong>程式碼的多少與模型的適用性和性能沒有直接關係。選擇模型應基於其解決問題的能力和對數據的擬合程度，而不是其實現的程式碼長短。</p></div>"
                            },
                            {
                                "question_text": "機器學習模型中的「超參數 (Hyperparameters)」與「參數 (Parameters)」有何不同？",
                                "options": {
                                    "A": "超參數是模型從數據中學習得到的，參數是在訓練前手動設定的",
                                    "B": "參數是模型從數據中學習得到的，超參數是在訓練前手動設定的，用於控制學習過程",
                                    "C": "兩者沒有本質區別，可以互換使用",
                                    "D": "超參數僅存在於深度學習模型中"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>在機器學習中，「參數」與「超參數」是兩個截然不同的概念：</p><p>- <strong>參數 (Parameters)</strong>：是模型內部的變量，在訓練過程中從數據中學習並優化得到的。例如，線性迴歸中的係數、神經網絡中的權重和偏置等。這些參數直接影響模型對新數據的預測結果，是通過最小化損失函數（如梯度下降）來自動求解的。</p><p>- <strong>超參數 (Hyperparameters)</strong>：是模型外部的設置，需要在訓練前由人工設定，用於控制學習算法本身的行為。超參數不是從數據中學習得到的，而是需要通過經驗、嘗試或系統性的調參技術（如網格搜索、隨機搜索）來確定。常見的超參數包括：學習率、正則化系數、神經網絡的層數和每層神經元數量、決策樹的最大深度、集成學習中的基學習器數量等。</p><p><strong>(A) 錯誤。</strong>該選項將參數和超參數的定義顛倒了。超參數是需要人工設定的，而不是從數據中學習得到的；參數則是模型在訓練過程中學習得到的，而不是手動設定的。</p><p><strong>(C) 錯誤。</strong>參數和超參數有本質的區別：一個是模型自動學習的，另一個是人工設定的；一個直接構成模型本身，另一個控制學習過程；它們絕不能互換使用。</p><p><strong>(D) 錯誤。</strong>超參數不僅存在於深度學習模型中，幾乎所有機器學習算法都有超參數。例如，決策樹的最大深度、支持向量機的核參數、K-近鄰的K值等，都是這些傳統機器學習算法的超參數。</p></div>"
                            },
                            {
                                "question_text": "奧卡姆剃刀原則 (Occam's Razor) 在模型選擇中的啟示是什麼？",
                                "options": {
                                    "A": "應選擇最複雜的模型，因為它能擬合更多細節",
                                    "B": "若有多個模型都能很好地解釋數據，應選擇結構最簡單的那個模型，以避免過擬合並提高泛化能力",
                                    "C": "模型的簡單與複雜程度與其性能無關",
                                    "D": "應選擇訓練數據量最大的模型"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>奧卡姆剃刀原則 (Occam's Razor) 是一個古老的哲學原則，其核心思想是「如無必要，勿增實體」(\"Entities should not be multiplied without necessity\")，或者更通俗地說，「若有多種方案，應選最簡單的那一個」。在機器學習和模型選擇中，這一原則的應用啟示是：</p><p>- 當有多個模型都能很好地擬合和解釋同一組數據時，應該選擇結構最簡單、最易解釋的那個模型。</p><p>- 越複雜的模型（例如，有更多參數、更深的層次）可能會帶來過擬合的風險，即模型過於複雜，不僅學習了數據中的真實模式，還學習了數據中的噪音和隨機波動，從而導致在新數據上的泛化性能下降。</p><p>- 簡單模型通常有更好的泛化能力，更健壯，更易於理解和解釋，計算成本更低，且不易出現過擬合。</p><p>在機器學習實踐中，這一原則常被用於模型正則化（如L1、L2正則化）和模型選擇（如基於交叉驗證的模型選擇）。</p><p><strong>(A) 錯誤。</strong>這與奧卡姆剃刀原則相反。最複雜的模型雖然可能在訓練數據上擬合得更好，但很可能會過擬合，導致在未見數據上表現不佳。奧卡姆剃刀原則提醒我們在解釋力相當的情況下，應選擇最簡單的模型。</p><p><strong>(C) 錯誤。</strong>模型的簡單與複雜程度與其性能有重要關係。過於簡單的模型可能欠擬合（無法捕捉數據中的複雜模式），而過於複雜的模型可能過擬合（學習了噪音，泛化能力差）。奧卡姆剃刀原則就是指導我們在這之間找到平衡點。</p><p><strong>(D) 錯誤。</strong>訓練數據量的大小與模型選擇是兩個不同的概念。奧卡姆剃刀原則涉及的是模型的複雜度，而不是數據量的大小。雖然更大的訓練數據集通常有助於訓練更複雜的模型而不至於過擬合，但這不是奧卡姆剃刀原則的核心啟示。</p></div>"
                            },
                            {
                                "question_text": "在設計深度神經網路架構時，增加網路的深度（層數）或寬度（每層神經元數量）可能會帶來什麼影響？",
                                "options": {
                                    "A": "總是能提高模型性能，且不會有任何負面影響",
                                    "B": "可能增強模型的表達能力以學習更複雜的模式，但也可能增加過擬合的風險和計算成本",
                                    "C": "只會增加計算成本，對模型性能沒有影響",
                                    "D": "會使模型更容易欠擬合"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>在設計深度神經網路架構時，增加網路的深度（層數）或寬度（每層神經元數量）會對模型產生多方面的影響：</p><p>- <strong>增強表達能力</strong>：更深或更寬的網路有更強的表達能力，能夠學習和建模更複雜的函數關係和數據模式。理論上，增加神經網路的深度可以使其有效地學習更加抽象和複雜的特徵層次結構。</p><p>- <strong>過擬合風險</strong>：然而，過於複雜的網路（太深或太寬）也更容易過擬合訓練數據，尤其是當訓練數據量有限時。過擬合的模型在訓練數據上表現很好，但在新的、未見過的數據上泛化能力差。</p><p>- <strong>計算成本</strong>：更深或更寬的網路意味著更多的參數和更複雜的計算圖，這會增加訓練和推斷的計算成本、記憶體需求和訓練時間。</p><p>- <strong>優化挑戰</strong>：非常深的網路可能面臨梯度消失或梯度爆炸等訓練難題，使得網路難以有效學習。雖然像批標準化、殘差連接這樣的技術可以緩解這些問題，但優化非常深的網路仍然是一個挑戰。</p><p>因此，在設計神經網路架構時，需要在模型複雜度（表達能力）和避免過擬合（泛化能力）之間找到平衡，同時考慮計算資源的限制和優化的可行性。實踐中，常常需要通過實驗和驗證來確定最適合特定問題的網路深度和寬度。</p><p><strong>(A) 錯誤。</strong>增加網路的深度或寬度並不總是能提高模型性能，尤其是當這導致過擬合或優化困難時。此外，增加計算成本、訓練時間、記憶體需求等也是明顯的負面影響。</p><p><strong>(C) 錯誤。</strong>增加網路的深度或寬度不僅會增加計算成本，還會影響模型的表達能力和性能。適當增加複雜度可以提升模型的能力，使其能夠學習更複雜的模式和特徵。</p><p><strong>(D) 錯誤。</strong>增加網路的深度或寬度通常會增加模型的表達能力，使其能夠擬合更複雜的函數。這反而會減少欠擬合的風險（欠擬合是指模型過於簡單，無法捕捉數據中的模式）。過度增加複雜度可能導致的問題是過擬合，而不是欠擬合。</p></div>"
                            },
                            {
                                "question_text": "當面臨一個新的機器學習問題時，一個好的起始策略是什麼？",
                                "options": {
                                    "A": "直接嘗試最複雜的深度學習模型",
                                    "B": "從一個或多個簡單的基線模型 (baseline model) 開始，逐步迭代，並根據評估結果決定是否嘗試更複雜的模型",
                                    "C": "花費所有時間在數據收集上，不進行模型實驗",
                                    "D": "隨機選擇一個模型進行訓練"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>在面對一個新的機器學習問題時，一個好的起始策略是從簡單開始，逐步迭代：</p><p>- <strong>建立基線模型 (baseline model)</strong>：首先實現一個或多個簡單、快速的基線模型，如線性迴歸、決策樹或簡單的神經網路。這些基線模型可以快速實現，提供初步的性能基準，並幫助理解問題的難度和特性。</p><p>- <strong>分析基線模型</strong>：評估基線模型的性能，分析其錯誤模式，理解模型的局限性，這可能揭示需要改進的方向（例如，需要更多特徵、更複雜的模型架構或更好的數據預處理）。</p><p>- <strong>逐步迭代</strong>：基於評估結果和分析，逐步改進模型，可能是添加更多特徵、調整超參數、嘗試不同的算法、增加模型複雜度等。每次迭代都應評估模型性能的變化，以確定改進是否有效。</p><p>- <strong>嘗試更複雜模型</strong>：只有當簡單模型已經被充分探索並顯示出局限性時，才考慮嘗試更複雜的模型。這種漸進式方法可以避免不必要的複雜性和計算資源浪費。</p><p>這種「由簡到繁」的迭代方法有助於更好地理解問題和數據，避免直接跳到複雜模型可能帶來的過擬合風險和解釋困難，同時也更有效率地利用時間和計算資源。</p><p><strong>(A) 錯誤。</strong>直接嘗試最複雜的深度學習模型不是一個好的起始策略，因為：1）複雜模型需要更多的數據、時間和計算資源；2）如果沒有適當的基線作為比較，難以評估複雜模型的效果是否真的必要；3）複雜模型更容易過擬合，特別是當數據量不足時；4）複雜模型的調試和解釋也更困難。</p><p><strong>(C) 錯誤。</strong>雖然高質量的數據對機器學習至關重要，但一個好的策略應該是數據收集和模型實驗的平衡。過早花費過多時間在數據收集上，而不進行任何模型實驗，可能會導致收集了不必要的或不適合問題的數據。透過早期的模型實驗，可以更好地理解哪些數據是真正有用的，哪些特徵是關鍵的。</p><p><strong>(D) 錯誤。</strong>隨機選擇一個模型進行訓練是一種非系統化的方法，可能會浪費時間和資源在不適合的模型上，而忽略了可能更適合的簡單解決方案。模型選擇應該基於問題類型、數據特性和性能要求等因素，而不是隨機選擇。</p></div>"
                            },
                            {
                                "question_text": "在機器學習中，將數據集劃分為訓練集 (Training Set)、驗證集 (Validation Set) 和測試集 (Test Set) 的目的是什麼？",
                                "options": {
                                    "A": "訓練集用於訓練模型，驗證集用於調整超參數和模型選擇，測試集用於最終評估模型的泛化能力",
                                    "B": "三個集合都可以混合用於模型訓練",
                                    "C": "驗證集和測試集是可選的，僅使用訓練集即可",
                                    "D": "測試集用於模型訓練，訓練集用於模型評估"
                                },
                                "correct_answer": "A",
                                "solution": "<div><p><strong>(A) 正確。</strong>在機器學習中，數據集通常被劃分為訓練集、驗證集和測試集，它們各自有不同的目的和用途：</p><p>- <strong>訓練集 (Training Set)</strong>：用於訓練模型，即模型從這部分數據中學習參數（例如，神經網路的權重、線性迴歸的係數）。訓練集通常佔據總數據集的最大部分（如70%）。</p><p>- <strong>驗證集 (Validation Set)</strong>：用於在訓練過程中評估模型性能，調整超參數（如學習率、正則化係數、模型架構等），以及在多個候選模型之間進行選擇。驗證集不參與模型的直接訓練，但其性能反饋會間接影響模型的選擇和優化。驗證集通常佔總數據集的一小部分（如15-20%）。</p><p>- <strong>測試集 (Test Set)</strong>：用於在模型訓練和選擇完成後，對最終選定的模型進行無偏的性能評估，估計模型在真實世界新數據上的泛化能力。測試集在整個訓練和調參過程中都不應該被使用，只在最後進行一次評估。測試集通常也佔總數據集的一小部分（如10-15%）。</p><p>這種劃分的主要目的是避免過擬合並獲得對模型泛化能力的可靠估計。如果僅使用同一個數據集進行訓練和評估，模型可能會過度適應該數據集的特性（包括其中的噪音），導致在新數據上表現不佳。通過使用獨立的驗證集調整超參數，使用獨立的測試集評估最終性能，可以更準確地了解模型在未見過的數據上的表現。</p><p><strong>(B) 錯誤。</strong>這三個集合有不同的用途，不應混合使用於模型訓練。訓練集用於學習模型參數，驗證集用於超參數調整和模型選擇，測試集用於最終的無偏評估。如果混合使用，就會導致「數據洩露」(data leakage)，使得模型可能過擬合，泛化能力評估也不可靠。</p><p><strong>(C) 錯誤。</strong>雖然在某些簡單的場景或資源有限的情況下，可能只使用訓練集和測試集（通過交叉驗證在訓練集中劃分出「折內驗證集」），但驗證集和測試集在嚴謹的機器學習實踐中是不可或缺的。僅使用訓練集既無法調整超參數，也無法可靠評估模型的泛化能力。</p><p><strong>(D) 錯誤。</strong>這與正確的數據劃分完全相反。測試集是用於最終評估模型，絕不應用於模型訓練，否則會導致嚴重的過擬合和評估偏差。</p></div>"
                            }
                        ]
                    },
                    "L23303": {
                        "title": "模型訓練、評估與驗證",
                        "questions": [
                            {
                                "question_text": "交叉驗證 (Cross-Validation) 在模型評估中的主要作用是什麼？",
                                "options": {
                                    "A": "加速模型訓練過程",
                                    "B": "減少訓練數據的需求量",
                                    "C": "提供對模型泛化能力的穩健估計，避免評估結果因數據劃分方式而產生波動",
                                    "D": "增加模型的參數數量"
                                },
                                "correct_answer": "C",
                                "solution": "<div><p><strong>(C) 正確。</strong>交叉驗證 (Cross-Validation) 是一種用於評估機器學習模型性能的統計方法，其主要目的是提供對模型泛化能力的穩健估計，避免評估結果因特定的訓練-測試數據劃分方式而產生偏差或波動。</p><p>在最常用的 k 折交叉驗證 (k-fold cross-validation) 中，數據集被分成 k 個大小相近的子集（折），每次使用 k-1 個子集作為訓練集，剩下的1個子集作為驗證集。這個過程重複 k 次，每次選擇不同的子集作為驗證集，最後取 k 次評估結果的平均值作為最終的性能估計。</p><p>交叉驗證的主要優點包括：</p><p>1. <strong>提供更穩健的模型評估</strong>：通過在不同的數據劃分上多次訓練和評估模型，可以減少由於特定的數據劃分帶來的評估偏差，得到更為穩健可靠的模型性能估計。</p><p>2. <strong>充分利用有限的數據</strong>：在數據量有限的情況下，交叉驗證可以確保所有數據都有機會用於訓練和評估，從而更充分地利用數據。</p><p>3. <strong>幫助檢測過擬合</strong>：如果模型在訓練集上表現很好但在交叉驗證中表現較差，這通常是過擬合的信號。</p><p>4. <strong>適用於模型選擇和超參數調整</strong>：可以使用交叉驗證來比較不同模型或同一模型的不同超參數設置，選擇最優的組合。</p><p><strong>(A) 錯誤。</strong>交叉驗證實際上可能會增加總體的計算時間，因為它需要多次訓練模型（例如，在 10 折交叉驗證中，需要訓練 10 個模型）。它的主要目的是為了更可靠地評估模型性能，而不是加速訓練過程。</p><p><strong>(B) 錯誤。</strong>交叉驗證不會減少訓練數據的需求量，它只是更有效地利用現有的數據。事實上，交叉驗證通常用於數據量有限時，因為它可以確保所有數據都用於訓練和評估。</p><p><strong>(D) 錯誤。</strong>交叉驗證是一種評估方法，不會改變模型本身的結構或參數數量。它只是在不同的數據劃分上多次訓練和評估同一個模型，以獲得更可靠的性能估計。</p></div>"
                            },
                            {
                                "question_text": "在機器學習中，什麼是「偏差-方差權衡」(Bias-Variance Tradeoff)？",
                                "options": {
                                    "A": "模型在訓練集和測試集上的性能差異",
                                    "B": "模型複雜度低時傾向於高偏差（欠擬合），復雜度高時傾向於高方差（過擬合），需在兩者間找到平衡點",
                                    "C": "訓練樣本大小與模型準確率的關係",
                                    "D": "不同評估指標間的取捨關係"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>「偏差-方差權衡」(Bias-Variance Tradeoff) 是機器學習中的一個核心概念，描述了模型預測誤差的兩個主要來源（偏差和方差）之間的此消彼長關係。</p><p>- <strong>偏差 (Bias)</strong>：指模型做出的假設與真實數據的基本關係的差距，即模型的簡化程度。高偏差的模型通常過於簡單，難以捕捉數據中的複雜模式，導致在訓練集和測試集上都表現不佳（欠擬合）。例如，用一個線性模型去擬合非線性數據。</p><p>- <strong>方差 (Variance)</strong>：指模型對訓練數據中隨機波動的敏感度。高方差的模型通常過於複雜，會學習訓練數據中的隨機噪聲，導致在訓練集上表現很好但在測試集上表現差（過擬合）。例如，一個高度複雜、能夠完美記住每個訓練樣本的模型。</p><p>「偏差-方差權衡」反映的是：</p><p>1. <strong>當模型複雜度增加（如增加多項式次數、樹的深度、神經網絡的層數等）時</strong>：</p><p>   - 模型的偏差通常會減少，因為更複雜的模型能夠學習更複雜的數據模式。</p><p>   - 但方差會增加，因為模型變得對訓練數據的波動更敏感。</p><p>2. <strong>當模型複雜度減少時</strong>：</p><p>   - 模型的偏差通常會增加，因為簡單模型可能無法捕捉複雜的數據模式。</p><p>   - 但方差會減少，因為模型對訓練數據的波動不那麼敏感。</p><p>機器學習中的一個關鍵挑戰是找到一個合適的平衡點，使得模型既不會過於簡單（高偏差），也不會過於複雜（高方差），從而最小化總體預測誤差。這通常通過調整模型的複雜度（例如，正則化參數、樹的深度、網絡架構等）並使用交叉驗證來尋找最佳的平衡點。</p><p><strong>(A) 錯誤。</strong>模型在訓練集和測試集上的性能差異通常被稱為「泛化差距」(generalization gap) 或「過擬合的程度」，而不是偏差-方差權衡。雖然這種差異與方差相關（高方差的模型通常在訓練集和測試集上的性能差異大），但偏差-方差權衡描述的是更基本的概念。</p><p><strong>(C) 錯誤。</strong>訓練樣本大小與模型準確率的關係通常被描述為學習曲線 (learning curve)，不是偏差-方差權衡。學習曲線可以用來診斷高偏差或高方差問題，但它描述的是不同現象。</p><p><strong>(D) 錯誤。</strong>不同評估指標間的取捨關係（如精確率和召回率之間的權衡）是另一種類型的權衡，與偏差-方差權衡無關。偏差-方差權衡描述的是模型預測誤差的兩個組成部分之間的關係。</p></div>"
                            },
                            {
                                "question_text": "在評估分類模型時，「混淆矩陣」(Confusion Matrix) 提供了哪些重要信息？",
                                "options": {
                                    "A": "僅顯示模型的準確率",
                                    "B": "僅顯示模型的訓練時間和預測時間",
                                    "C": "顯示真陽性、假陽性、真陰性和假陰性的數量，有助於計算各種性能指標如精確率、召回率等",
                                    "D": "顯示模型的計算複雜度"
                                },
                                "correct_answer": "C",
                                "solution": "<div><p><strong>(C) 正確。</strong>混淆矩陣 (Confusion Matrix) 是評估分類模型性能的一個重要工具，它以表格形式呈現模型的預測結果與實際標籤的對比。在二元分類（二分類）問題中，混淆矩陣是一個 2×2 的表格，包含以下四種情況：</p><p>- <strong>真陽性 (True Positive, TP)</strong>：模型正確地預測正類別為正類別。例如，正確識別出患有某疾病的病人。</p><p>- <strong>假陽性 (False Positive, FP)</strong>：模型錯誤地將負類別預測為正類別。也被稱為「第一類錯誤」或「假警報」。例如，將健康人誤診為患病。</p><p>- <strong>真陰性 (True Negative, TN)</strong>：模型正確地預測負類別為負類別。例如，正確識別出健康人。</p><p>- <strong>假陰性 (False Negative, FN)</strong>：模型錯誤地將正類別預測為負類別。也被稱為「第二類錯誤」或「漏報」。例如，將患病人誤診為健康。</p><p>基於混淆矩陣，可以計算多種性能指標，包括：</p><p>- <strong>準確率 (Accuracy)</strong> = (TP + TN) / (TP + FP + TN + FN)：正確預測的樣本比例。</p><p>- <strong>精確率 (Precision)</strong> = TP / (TP + FP)：在所有被預測為正類別的樣本中，真正屬於正類別的比例。衡量模型的準確性。</p><p>- <strong>召回率 (Recall)</strong> = TP / (TP + FN)：在所有實際為正類別的樣本中，被正確識別的比例。也稱為敏感度 (Sensitivity)，衡量模型的完備性。</p><p>- <strong>特異度 (Specificity)</strong> = TN / (TN + FP)：在所有實際為負類別的樣本中，被正確識別的比例。</p><p>- <strong>F1分數 (F1 Score)</strong> = 2 * (Precision * Recall) / (Precision + Recall)：精確率和召回率的調和平均數，當類別不平衡時特別有用。</p><p>混淆矩陣在多分類問題中可以擴展為 n×n 的矩陣，其中 n 是類別的數量。</p><p><strong>(A) 錯誤。</strong>混淆矩陣不僅顯示模型的準確率，它提供了更詳細的信息，允許計算多種性能指標（如前所述）。準確率只是混淆矩陣能夠提供的眾多指標之一。</p><p><strong>(B) 錯誤。</strong>混淆矩陣與模型的訓練時間和預測時間無關。它只關注模型的預測結果與真實標籤的對比。</p><p><strong>(D) 錯誤。</strong>混淆矩陣不顯示模型的計算複雜度。計算複雜度是指算法執行所需的時間和空間資源，與混淆矩陣無關。混淆矩陣專注於模型的預測性能評估。</p></div>"
                            },
                            {
                                "question_text": "在訓練機器學習模型時，如何判斷和處理過擬合 (Overfitting) 問題？",
                                "options": {
                                    "A": "過擬合是訓練集和測試集性能差距過大的現象，可通過正則化、增加數據、降低模型複雜度等方式減輕",
                                    "B": "過擬合指模型在訓練和測試數據上都表現不佳，可通過增加模型複雜度處理",
                                    "C": "過擬合是正常現象，不需特別處理",
                                    "D": "過擬合指模型訓練速度過慢，可通過增加學習率解決"
                                },
                                "correct_answer": "A",
                                "solution": "<div><p><strong>(A) 正確。</strong>過擬合 (Overfitting) 是指機器學習模型在訓練數據上表現很好，但在新的、未見過的數據（如測試集）上表現較差的現象。這種現象通常表現為訓練集和測試集的性能指標（如準確率、誤差）之間存在顯著差距。過擬合發生的根本原因是模型學習了訓練數據中的噪聲和隨機波動，而不僅僅是底層的數據模式，從而無法很好地泛化到新數據。</p><p><strong>判斷過擬合的方法</strong>：</p><p>1. <strong>訓練集和驗證集/測試集性能差距大</strong>：模型在訓練集上的性能明顯優於在驗證集或測試集上的性能。</p><p>2. <strong>學習曲線分析</strong>：訓練誤差隨著訓練迭代而持續下降，但驗證誤差在某一點後開始上升。</p><p>3. <strong>模型複雜度與性能的關係</strong>：隨著模型複雜度增加，驗證性能先提升後下降。</p><p><strong>處理過擬合的常見方法</strong>：</p><p>1. <strong>正則化 (Regularization)</strong>：例如L1、L2正則化，通過在損失函數中添加懲罰項來限制模型參數的大小，從而減少模型的複雜度。</p><p>2. <strong>增加訓練數據量</strong>：更多的訓練數據使模型更難「記住」每個樣本，從而促使它學習更一般的模式。</p><p>3. <strong>降低模型複雜度</strong>：例如，減少神經網絡的層數或神經元數量，降低決策樹的深度等。</p><p>4. <strong>早停 (Early Stopping)</strong>：在模型開始在驗證集上表現變差之前停止訓練。</p><p>5. <strong>Dropout</strong>：在深度學習中，通過在訓練過程中隨機「關閉」一部分神經元來減少神經元之間的相互依賴。</p><p>6. <strong>數據增強 (Data Augmentation)</strong>：通過對現有訓練數據進行變換（如旋轉、縮放、添加噪聲等）來人為增加訓練樣本的多樣性。</p><p>7. <strong>集成學習 (Ensemble Learning)</strong>：結合多個模型的預測結果，減少單個模型過擬合的影響。</p><p><strong>(B) 錯誤。</strong>過擬合是指模型在訓練數據上表現良好但在測試數據上表現較差，而不是兩者都表現不佳。如果模型在訓練和測試數據上都表現不佳，這通常是欠擬合 (Underfitting) 的跡象，欠擬合的解決方法可能包括增加模型複雜度、減少正則化強度等。</p><p><strong>(C) 錯誤。</strong>過擬合不是正常現象，而是一個需要解決的問題。過擬合的模型在實際應用中可能表現很差，因為它無法很好地泛化到新的數據。</p><p><strong>(D) 錯誤。</strong>過擬合與模型訓練速度無關，而是指模型對訓練數據擬合過度，失去了泛化能力。增加學習率可能會影響訓練速度和收斂性，但不是解決過擬合的方法（實際上，在某些情況下，過高的學習率甚至可能導致訓練不穩定）。</p></div>"
                            },
                            {
                                "question_text": "模型評估中的ROC曲線 (Receiver Operating Characteristic curve) 和AUC (Area Under the Curve) 指標主要用來衡量什麼？",
                                "options": {
                                    "A": "模型的計算效率",
                                    "B": "模型在不同閾值下的分類性能，特別是權衡真陽性率和假陽性率的能力",
                                    "C": "模型的訓練時間",
                                    "D": "模型參數的數量"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>ROC曲線 (Receiver Operating Characteristic curve) 和AUC (Area Under the Curve) 是評估二元分類模型性能的重要工具，特別是在需要權衡不同類型錯誤的情況下。</p><p><strong>ROC曲線</strong>：是以不同分類閾值下的真陽性率 (True Positive Rate, TPR) 為縱軸，假陽性率 (False Positive Rate, FPR) 為橫軸繪製的曲線。其中：</p><p>- 真陽性率 (TPR)，也稱為敏感度 (Sensitivity) 或召回率 (Recall)：= TP / (TP + FN)，表示實際為正類的樣本被正確識別的比例。</p><p>- 假陽性率 (FPR) = FP / (FP + TN)，表示實際為負類的樣本被錯誤地識別為正類的比例。</p><p>ROC曲線的每一點對應於一個特定的分類閾值：隨著閾值從嚴格到寬鬆的變化，TPR和FPR都會增加，形成一條從左下角 (0,0) 到右上角 (1,1) 的曲線。</p><p><strong>AUC</strong>：是ROC曲線下的面積，取值範圍在0到1之間。AUC = 1表示完美分類器，能夠100%正確地將所有正例和負例區分開；AUC = 0.5表示一個隨機猜測的分類器，沒有任何區分能力；AUC < 0.5表示比隨機猜測還差的分類器。</p><p>ROC曲線和AUC的主要用途和優點包括：</p><p>1. <strong>評估模型在不同閾值下的性能</strong>：ROC曲線展示了模型在所有可能閾值下的性能表現，幫助選擇最適合特定問題的閾值。</p><p>2. <strong>對類別不平衡問題不敏感</strong>：與準確率等指標不同，ROC曲線和AUC不受類別分佈變化的影響，在處理不平衡數據集時特別有用。</p><p>3. <strong>提供對模型區分能力的整體評估</strong>：AUC值可以看作是模型將隨機選擇的正例樣本排在隨機選擇的負例樣本前面的概率，因此它衡量的是模型的整體排序能力。</p><p><strong>(A) 錯誤。</strong>ROC曲線和AUC與模型的計算效率無關，它們只關注模型的預測性能，特別是在不同閾值下對正例和負例的區分能力。</p><p><strong>(C) 錯誤。</strong>ROC曲線和AUC不衡量模型的訓練時間。模型訓練時間是一個運行效率指標，通常以時間單位（如秒、分鐘）來衡量，而ROC曲線和AUC是性能評估指標。</p><p><strong>(D) 錯誤。</strong>ROC曲線和AUC不衡量模型參數的數量。模型參數的數量是模型複雜度的一個度量，與ROC曲線和AUC這種評估預測性能的指標沒有直接關係。</p></div>"
                            }
                        ]
                    }
                }
            },
            "L234": {
                "title": "機器學習治理",
                "sub_sections": {
                    "L23401": {
                        "title": "數據隱私、安全與合規",
                        "questions": []
                    },
                    "L23402": {
                        "title": "演算法偏見與公平性",
                        "questions": []
                    }
                }
            }
        }
    }
}