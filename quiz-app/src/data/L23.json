{
    "L23": {
        "title": "機器學習技術與應用",
        "sections": {
            "L231": {
                "title": "機器學習基礎數學",
                "sub_sections": {
                    "L23101": {
                        "title": "機率/統計之機器學習基礎應用",
                        "questions": [
                            {
                                "question_text": "在機器學習中，貝氏定理 (Bayes' Theorem) 主要用於什麼情境？",
                                "options": {
                                    "A": "計算數據的平均值與變異 (Variance)數 (Variance)",
                                    "B": "根據新的證據更新事件發生的機率 (計算後驗機率 (Posterior Probability))",
                                    "C": "進行線性迴歸 (Regression)模型的參數估計",
                                    "D": "判斷兩組數據的平均數 (Mean)是否有顯著差異"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>貝氏定理 (Bayes' Theorem) 描述了在已知某些條件的情況下，某一事件發生的機率。在機器學習中，它常被用於根據觀測到的新證據（數據）來更新我們對某個假設或事件發生機率的信念，即計算後驗機率 (Posterior Probability)。例如，在樸素貝氏分類器中，就是利用貝氏定理來預測樣本屬於特定類別的後驗機率。</p><p><strong>(A) 錯誤。</strong>計算數據的平均值與變異數屬於描述性統計 (Descriptive Statistics) 的範疇，用於總結數據的基本特徵，與貝氏定理直接更新機率的功能不同。</p><p><strong>(C) 錯誤。</strong>進行線性迴歸模型的參數估計通常使用最小平方法 (Least Squares Method) 或最大概似估計 (Maximum Likelihood Estimation) 等方法，而非直接應用貝氏定理來估計參數（雖然貝氏線性迴歸會用到貝氏方法，但題目問的是貝氏定理的主要用途）。</p><p><strong>(D) 錯誤。</strong>判斷兩組數據的平均數是否有顯著差異通常使用假設檢定中的 t-檢定 (t-test) 或 ANOVA 等統計方法，與貝氏定理更新機率的核心思想不同。</p></div>"
                            },
                            {
                                "question_text": "最大概似估計 (Maximum Likelihood Estimation) (Maximum Likelihood Estimation, MLE (Maximum Likelihood Estimation)) 的核心思想是什麼？",
                                "options": {
                                    "A": "選擇一組參數，使得在這些參數下，觀測到目前樣本數據的機率最小",
                                    "B": "選擇一組參數，使得在這些參數下，觀測到目前樣本數據的機率最大",
                                    "C": "選擇一組參數，使得模型的複雜度最低",
                                    "D": "選擇一組參數，使得模型的預測誤差最小"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>最大概似估計 (Maximum Likelihood Estimation, MLE) 是一種在給定觀測數據的情況下，估計模型參數的方法。其核心思想是：選擇一組能夠使得「已觀測到的樣本數據出現的機率（即概似函數值）」最大的參數值，作為參數的最佳估計。換句話說，我們認為最合理的參數估計，應該是那個最能解釋我們手中現有數據的參數。</p><p><strong>(A) 錯誤。</strong>這與MLE的核心思想相反。MLE旨在最大化觀測數據出現的機率，而非最小化。</p><p><strong>(C) 錯誤。</strong>選擇使得模型複雜度最低的原則通常與奧卡姆剃刀原則 (Occam's Razor) 或正規化 (Regularization) 方法相關，旨在防止過擬合，而非MLE的核心思想。MLE本身不直接考慮模型複雜度，而是專注於數據的概似程度。</p><p><strong>(D) 錯誤。</strong>選擇使得模型的預測誤差最小是許多機器學習模型訓練的目標，例如透過最小化損失函數（如均方誤差、交叉熵）來實現。雖然MLE的結果往往能導向預測誤差較小的模型，但其直接優化的目標是概似函數，而非預測誤差本身。</p></div>"
                            },
                            {
                                "question_text": "在假設檢定中，若p值 (p-value) 小於顯著水準 (significance level) (significance level, α)，通常會做出何種決策？",
                                "options": {
                                    "A": "接受虛無假設 (Null Hypothesis)",
                                    "B": "拒絕虛無假設 (Null Hypothesis)，接受對立假設 (Alternative Hypothesis)",
                                    "C": "增加樣本數重新進行檢定",
                                    "D": "p值 (p-value)與顯著水準 (significance level)無關"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>在假設檢定中，p值 (p-value) 是指在虛無假設 (Null Hypothesis, H0) 為真的前提下，觀測到目前樣本結果或更極端結果的機率。顯著水準 (significance level, α) 是一個預先設定的閾值（通常是0.05或0.01），代表我們願意承擔的犯第一型錯誤（即錯誤地拒絕一個真實的虛無假設）的最大機率。當計算出來的p值小於顯著水準α時，表示觀測到的數據與虛無假設的預期差異足夠大，這種差異不太可能是由隨機抽樣造成的。因此，我們有足夠的證據拒絕虛無假設，並接受對立假設 (Alternative Hypothesis, H1 或 Ha)。</p><p><strong>(A) 錯誤。</strong>接受虛無假設是在p值大於或等於顯著水準α時的決策，表示沒有足夠的證據拒絕虛無假設。</p><p><strong>(C) 錯誤。</strong>雖然增加樣本數可能會影響檢定結果（例如增加檢定力），但當p值小於α時，標準的決策是拒絕虛無假設，而不是立即要求增加樣本數。是否需要增加樣本數是另一個層面的考量，例如檢定力不足或結果不夠精確時。</p><p><strong>(D) 錯誤。</strong>p值與顯著水準密切相關，它們是做出統計決策的關鍵依據。決策規則就是比較p值和α的大小。</p></div>"
                            },
                            {
                                "question_text": "下列何種機率分佈常用於描述在固定時間間隔或空間區域內，某事件發生次數的機率？",
                                "options": {
                                    "A": "常態分佈 (Normal Distribution)",
                                    "B": "伯努利分佈 (Bernoulli Distribution)",
                                    "C": "二項分佈 (Binomial Distribution)",
                                    "D": "泊松分佈 (Poisson Distribution)"
                                },
                                "correct_answer": "D",
                                "solution": "<div><p><strong>(D) 正確。</strong>泊松分佈 (Poisson Distribution) 是一種離散機率分佈，用於描述在一個固定的時間間隔、空間區域、或指定的單位內，某一獨立事件發生的平均次數已知（通常用λ表示）的情況下，該事件實際發生k次的機率。例如，一小時內到達某服務台的顧客人數、一頁書中的錯字數量等，都可能服從泊松分佈。</p><p><strong>(A) 錯誤。</strong>常態分佈 (Normal Distribution)，也稱高斯分佈，是一種連續機率分佈，描述了許多自然現象中連續變量的分佈情況，其圖形呈鐘形。它與描述固定區間內事件發生次數的泊松分佈不同。</p><p><strong>(B) 錯誤。</strong>伯努利分佈 (Bernoulli Distribution) 是一種離散機率分佈，描述的是單次隨機試驗的結果，該試驗只有兩種可能的結果（例如成功/失敗，正面/反面），通常用0和1表示。它關注的是單次試驗，而非固定區間內的發生次數。</p><p><strong>(C) 錯誤。</strong>二項分佈 (Binomial Distribution) 是一種離散機率分佈，描述的是在n次獨立的伯努利試驗中，成功發生k次的機率，其中每次試驗成功的機率p保持不變。它關注的是固定次數試驗中的成功次數，而泊松分佈關注的是固定區間內的事件發生次數，且泊松分佈可以看作是當n很大、p很小時二項分佈的極限情況。</p></div>"
                            }
                        ]
                    },
                    "L23103": {
                        "title": "數值優化技術與方法",
                        "questions": [
                            {
                                "question_text": "梯度下降 (Gradient Descent)法 (Gradient Descent) 是一種常用的優化演算法，其更新參數的主要依據是什麼？",
                                "options": {
                                    "A": "損失函數 (Loss Function)的二階導數 (Hessian矩陣 (Hessian matrix))",
                                    "B": "損失函數 (Loss Function)對於模型參數的梯度 (一階導數)",
                                    "C": "隨機選擇一個方向進行更新",
                                    "D": "損失函數 (Loss Function)本身的值"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>梯度下降法 (Gradient Descent) 是一種迭代優化演算法，用於尋找函數的局部最小值。在機器學習中，這個函數通常是損失函數 (Loss Function)，而我們希望找到使損失函數最小化的模型參數。梯度是函數在某一點上變化最快（上升最快）的方向，其負方向則是函數下降最快的方向。因此，梯度下降法透過計算損失函數對於模型參數的梯度（即一階偏導數向量），然後沿著梯度的負方向按一定步長（學習率）更新參數，從而逐步逼近損失函數的最小值點。</p><p><strong>(A) 錯誤。</strong>損失函數的二階導數（Hessian矩陣）被用於更高級的優化方法，如牛頓法 (Newton's method) 或擬牛頓法 (Quasi-Newton methods)。這些方法可以利用曲率信息來加速收斂，但梯度下降法本身主要依賴一階導數（梯度）。</p><p><strong>(C) 錯誤。</strong>隨機選擇一個方向進行更新的方法，如隨機搜索 (Random Search)，通常效率較低，且不保證能找到最優解。梯度下降法是基於梯度的確定性方向（或其估計）進行更新的。</p><p><strong>(D) 錯誤。</strong>雖然損失函數本身的值用於評估當前參數的好壞，但參數更新的方向是由梯度決定的，而不是直接由損失函數的值決定的。梯度指明了如何調整參數以最大程度地減少損失。</p></div>"
                            },
                            {
                                "question_text": "在梯度下降 (Gradient Descent)法 (Gradient Descent)中，學習率 (learning rate) 的選擇有何影響？",
                                "options": {
                                    "A": "學習率 (learning rate)越大，收斂速度一定越快且結果越好",
                                    "B": "學習率 (learning rate)越小，收斂速度一定越慢但結果越好",
                                    "C": "學習率 (learning rate)過大可能導致震盪或無法收斂；學習率 (learning rate)過小可能導致收斂速度過慢",
                                    "D": "學習率 (learning rate)對模型訓練沒有實質影響"
                                },
                                "correct_answer": "C",
                                "solution": "<div><p><strong>(C) 正確。</strong>學習率 (learning rate) 是梯度下降法中的一個關鍵超參數，它控制了每一步參數更新的幅度（即沿著梯度負方向移動的距離）。學習率的選擇對模型的訓練過程和最終性能有顯著影響：</p><p>-   <strong>學習率過大</strong>：如果學習率設置得太大，參數更新的步長可能過大，導致在損失函數的谷底附近來回震盪，甚至越過最小值點，使得損失函數不降反升，最終無法收斂到最優解。</p><p>-   <strong>學習率過小</strong>：如果學習率設置得太小，參數更新的步長會非常緩慢，導致模型收斂到最優解的速度非常慢，需要更多的迭代次數，增加了訓練時間。</p><p>因此，選擇一個合適的學習率至關重要，它需要在收斂速度和收斂穩定性之間取得平衡。</p><p><strong>(A) 錯誤。</strong>學習率越大，收斂速度不一定越快，如果過大反而可能導致不收斂或震盪，結果也可能更差。</p><p><strong>(B) 錯誤。</strong>學習率越小，收斂速度確實會變慢，但並不保證結果一定越好。過小的學習率可能導致模型陷入局部最小值，或者需要極長的訓練時間才能達到一個好的解。</p><p><strong>(D) 錯誤。</strong>學習率對模型訓練有非常實質的影響，是梯度下降法能否成功找到最優解的關鍵因素之一。</p></div>"
                            },
                            {
                                "question_text": "隨機梯度下降 (Gradient Descent)法 (Stochastic Gradient Descent) (Stochastic Gradient Descent, SGD (Stochastic Gradient Descent)) 與批次梯度下降 (Gradient Descent)法 (Batch Gradient Descent) 的主要區別是什麼？",
                                "options": {
                                    "A": "SGD (Stochastic Gradient Descent) 使用整個訓練集的梯度來更新參數，BGD (Batch Gradient Descent) 使用單一樣本",
                                    "B": "SGD (Stochastic Gradient Descent) 每次使用單一或小批量樣本的梯度來更新參數，BGD (Batch Gradient Descent) 使用整個訓練集",
                                    "C": "SGD (Stochastic Gradient Descent) 的計算成本通常高於 BGD (Batch Gradient Descent)",
                                    "D": "BGD (Batch Gradient Descent) 的收斂路徑通常比 SGD (Stochastic Gradient Descent) 更不穩定"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>梯度下降法有幾種變體，主要區別在於每次參數更新時使用的數據量：</p><p>-   <strong>批次梯度下降法 (Batch Gradient Descent, BGD)</strong>：在每次參數更新時，使用整個訓練集的數據來計算損失函數的梯度。這種方法計算的梯度更準確，收斂路徑更穩定，但當訓練集非常大時，每次迭代的計算成本很高，且可能難以處理無法一次性載入記憶體的數據集。</p><p>-   <strong>隨機梯度下降法 (Stochastic Gradient Descent, SGD)</strong>：在每次參數更新時，僅使用訓練集中的一個隨機選擇的樣本來計算梯度。這種方法每次迭代的計算成本非常低，更新速度快，並且其隨機性有助於跳出局部最小值。然而，由於每次只用一個樣本，梯度估計的變異較大，導致收斂路徑較為震盪。</p><p>-   <strong>小批量梯度下降法 (Mini-batch Gradient Descent)</strong>：這是介於BGD和SGD之間的一種折衷方法。它在每次參數更新時，使用訓練集中的一小部分隨機樣本（一個mini-batch）來計算梯度。這種方法結合了BGD的穩定性和SGD的效率，是目前深度學習中最常用的梯度下降變體。</p><p>題目中的SGD指的是使用單一或小批量樣本，與BGD使用整個訓練集形成對比。</p><p><strong>(A) 錯誤。</strong>這描述反了。BGD使用整個訓練集，SGD使用單一（或小批量）樣本。</p><p><strong>(C) 錯誤。</strong>SGD（尤其是單樣本SGD）每次迭代的計算成本遠低於BGD，因為它只需要處理一個樣本而不是整個訓練集。即使是Mini-batch SGD，其計算成本也通常低於BGD（除非mini-batch大小等於整個訓練集大小）。</p><p><strong>(D) 錯誤。</strong>由於BGD使用整個訓練集的梯度，其收斂路徑通常比SGD更平滑、更穩定。SGD由於每次使用隨機樣本估計梯度，其收斂路徑會更加震盪和不穩定。</p></div>"
                            },
                            {
                                "question_text": "在機器學習中，損失函數 (Loss Function) 的主要作用是什麼？",
                                "options": {
                                    "A": "衡量模型預測的準確程度，數值越大代表模型越好",
                                    "B": "衡量模型預測值與真實值之間的差異，優化的目標是最小化損失函數 (Loss Function)",
                                    "C": "用於對輸入數據進行特徵提取 (Feature Extraction)",
                                    "D": "決定模型架構的複雜度"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>損失函數（也稱為成本函數或目標函數）在機器學習中扮演著核心角色。它的主要作用是量化模型預測結果與實際真實值（標籤）之間的差異或「損失」。一個好的模型應該使其預測盡可能接近真實值，因此損失函數的值越小，通常表示模型的性能越好。在模型訓練過程中，優化的目標就是調整模型的參數（例如神經網路的權重），以最小化損失函數的值。</p><p><strong>(A) 錯誤。</strong>損失函數衡量的是模型的「錯誤」或「損失」程度，因此其數值越小代表模型越好，而非越大越好。準確度 (Accuracy) 等指標才是數值越大代表模型越好。</p><p><strong>(C) 錯誤。</strong>特徵提取 (Feature Extraction) 是數據預處理或模型架構的一部分（例如CNN中的卷積層），旨在從原始數據中抽取出更有代表性、更能區分不同類別的特徵。損失函數是用於評估和指導模型學習這些特徵（以及後續的分類/迴歸任務）的好壞，而不是直接進行特徵提取。</p><p><strong>(D) 錯誤。</strong>決定模型架構的複雜度（例如神經網路的層數、神經元數量，或決策樹的深度等）是模型設計階段的任務，通常由開發者根據問題特性、數據量和計算資源等因素來決定。損失函數本身不決定模型架構的複雜度，但模型的複雜度會影響其最小化損失函數的能力以及是否容易過擬合。</p></div>"
                            }
                        ]
                    }
                }
            },
            "L232": {
                "title": "機器學習與深度學習",
                "sub_sections": {
                    "L23201": {
                        "title": "機器學習原理與技術",
                        "questions": [
                            {
                                "question_text": "下列何者是監督式學習 (Supervised Learning) 的主要特點？",
                                "options": {
                                    "A": "訓練數據沒有標籤，模型需要自行找出數據中的結構",
                                    "B": "訓練數據包含輸入特徵以及對應的正確輸出標籤",
                                    "C": "模型透過與環境互動，學習最大化獎勵的策略",
                                    "D": "主要用於數據降維 (Dimensionality Reduction)與視覺化"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>監督式學習 (Supervised Learning) 是機器學習的一大類別，其核心特點是使用帶有標籤 (labeled) 的訓練數據。這意味著提供給模型的每個輸入樣本（由一組特徵描述）都配有一個已知的、正確的輸出結果或目標值（即標籤）。模型的任務是學習從輸入特徵到輸出標籤之間的映射關係，以便能夠對新的、未見過的輸入數據進行準確的預測。常見的監督式學習任務包括分類 (Classification) 和迴歸 (Regression)。</p><p><strong>(A) 錯誤。</strong>訓練數據沒有標籤，模型需要自行找出數據中的內在結構、模式或分組，這描述的是非監督式學習 (Unsupervised Learning) 的主要特點，例如分群 (Clustering) 或關聯規則學習 (Association Rule Learning)。</p><p><strong>(C) 錯誤。</strong>模型透過與環境互動，並根據收到的獎勵或懲罰來學習最佳行動策略，以最大化長期累積獎勵，這描述的是強化學習 (Reinforcement Learning) 的主要特點。</p><p><strong>(D) 錯誤。</strong>數據降維 (Dimensionality Reduction)，如主成分分析 (PCA)，以及數據視覺化，通常屬於非監督式學習的範疇，因為它們旨在探索數據的結構或以更簡潔的方式表示數據，而不一定需要預先定義的標籤。雖然監督式方法也可以用於特徵選擇（一種形式的降維），但「主要用於數據降維與視覺化」更符合非監督式學習的描述。</p></div>"
                            },
                            {
                                "question_text": "當一個機器學習模型在訓練數據上表現良好，但在未見過的測試數據上表現很差時，這種現象稱為什麼？",
                                "options": {
                                    "A": "欠擬合 (Underfitting)",
                                    "B": "過擬合 (Overfitting)",
                                    "C": "偏差 (Bias)",
                                    "D": "變異 (Variance)"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>過擬合 (Overfitting) 是指機器學習模型在訓練數據上學到了過多的細節和噪聲，導致其對訓練數據的擬合非常好（例如，訓練誤差很低），但在新的、未見過的數據（如測試數據或實際應用中的數據）上表現不佳（例如，測試誤差很高）。模型失去了泛化能力，無法很好地適應新的數據。這通常發生在模型過於複雜（相對於數據量和數據的真實複雜度而言）或者訓練時間過長的情況下。</p><p><strong>(A) 錯誤。</strong>欠擬合 (Underfitting) 是指模型過於簡單，無法捕捉到數據中的基本模式和規律，導致其在訓練數據上和測試數據上都表現不佳。模型沒有充分學習數據的特性。</p><p><strong>(C) 錯誤。</strong>偏差 (Bias) 是指模型預測值的期望與真實值之間的差異，衡量的是模型的擬合能力。高偏差通常意味著模型欠擬合，無法很好地捕捉數據的真實關係。雖然過擬合與偏差和變異有關（通常是低偏差、高變異），但題目描述的現象直接指的是過擬合。</p><p><strong>(D) 錯誤。</strong>變異 (Variance) 是指模型在不同訓練數據集上訓練時，其預測結果的變化程度，衡量的是模型對訓練數據變化的敏感性。高變異通常意味著模型過擬合，對訓練數據中的噪聲過於敏感。與偏差類似，雖然過擬合通常伴隨著高變異，但題目描述的現象本身被稱為過擬合。</p></div>"
                            },
                            {
                                "question_text": "交叉驗證 (Cross-Validation) 在機器學習中的主要目的是什麼？",
                                "options": {
                                    "A": "增加訓練數據的數量",
                                    "B": "更可靠地評估模型的泛化能力，並輔助模型選擇與超參數調整",
                                    "C": "加速模型的訓練過程",
                                    "D": "降低模型的複雜度"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>交叉驗證 (Cross-Validation) 是一種評估機器學習模型泛化能力的統計方法。它將原始數據集劃分為多個子集（或稱為「摺」，folds），然後輪流使用其中一個子集作為驗證集（用於評估模型），其餘子集作為訓練集（用於訓練模型）。這個過程重複多次，每次使用不同的子集作為驗證集。最後，將多次評估的結果（例如準確率、誤差）平均起來，得到一個更穩定、更可靠的模型性能估計。交叉驗證的主要目的包括：</p><p>-   <strong>更可靠地評估模型的泛化能力</strong>：相比於單次劃分訓練集和測試集，交叉驗證可以減少因數據劃分的隨機性帶來的評估偏差，從而更準確地反映模型在未見數據上的表現。</p><p>-   <strong>輔助模型選擇與超參數調整</strong>：在比較不同模型或調整模型超參數時，可以使用交叉驗證來選擇在驗證集上平均表現最好的模型或超參數組合，以避免過擬合到特定的驗證集。</p><p><strong>(A) 錯誤。</strong>交叉驗證本身並不增加訓練數據的總量，它只是更有效地利用現有的數據進行模型訓練和評估。數據增強 (Data Augmentation) 等技術才是用於增加訓練數據數量的方法。</p><p><strong>(C) 錯誤。</strong>交叉驗證通常會增加總體的計算時間，因為模型需要被訓練和評估多次（例如，k-摺交叉驗證需要訓練k次模型）。它並不能加速單個模型的訓練過程。</p><p><strong>(D) 錯誤。</strong>降低模型的複雜度通常是透過正規化 (Regularization)、剪枝 (Pruning，如決策樹) 或選擇更簡單的模型架構來實現的，目的是防止過擬合。交叉驗證本身是評估工具，雖然其結果可能引導我們選擇一個複雜度較低的模型（如果該模型泛化能力更好），但它不直接降低模型的複雜度。</p></div>"
                            },
                            {
                                "question_text": "偏差 (Bias)-變異 (Variance)權衡 (Bias-Variance Tradeoff) 描述了機器學習模型中的哪種關係？",
                                "options": {
                                    "A": "模型的偏差 (Bias)越高，變異 (Variance)通常也越高",
                                    "B": "模型的偏差 (Bias)越低，變異 (Variance)通常也越低",
                                    "C": "降低模型的偏差 (Bias)可能會導致變異 (Variance)增加，反之亦然，需要在兩者間取得平衡",
                                    "D": "偏差 (Bias)與變異 (Variance)是完全獨立的，不存在權衡關係"
                                },
                                "correct_answer": "C",
                                "solution": "<div><p><strong>(C) 正確。</strong>偏差-變異權衡 (Bias-Variance Tradeoff) 是機器學習中一個核心概念，描述了模型預測誤差的兩個主要來源之間的關係：</p><p>-   <strong>偏差 (Bias)</strong>：指模型預測值的期望與真實值之間的差異。高偏差意味著模型對數據的假設過於簡單，未能捕捉到數據的真實規律，導致欠擬合 (Underfitting)。</p><p>-   <strong>變異 (Variance)</strong>：指模型在不同訓練數據集上訓練時，其預測結果的變化程度或不穩定性。高變異意味著模型對訓練數據中的噪聲或隨機波動過於敏感，導致過擬合 (Overfitting)。</p><p>偏差-變異權衡指出，通常情況下，當我們試圖降低模型的偏差時（例如，使用更複雜的模型），其變異可能會增加；反之，當我們試圖降低模型的變異時（例如，使用更簡單的模型或正規化），其偏差可能會增加。因此，在模型選擇和訓練過程中，需要在偏差和變異之間找到一個平衡點，以最小化總體的預期泛化誤差。</p><p><strong>(A) 錯誤。</strong>高偏差（欠擬合）的模型通常對數據不敏感，因此其變異可能較低。而高變異（過擬合）的模型通常能夠很好地擬合訓練數據，偏差可能較低。</p><p><strong>(B) 錯誤。</strong>理想情況下我們希望偏差和變異都低，但實際中兩者往往存在權衡關係，很難同時達到最低。</p><p><strong>(D) 錯誤。</strong>偏差與變異並非完全獨立，它們之間存在著此消彼長的權衡關係，是影響模型泛化能力的兩個重要且相互關聯的因素。</p></div>"
                            },
                            {
                                "question_text": "集成學習 (Ensemble Learning) 方法，如隨機森林和梯度提升樹，其核心思想是什麼？",
                                "options": {
                                    "A": "只使用單一且最強大的模型進行預測",
                                    "B": "將多個弱學習器 (weak learners) 組合起來，形成一個更強大、更穩健的學習器",
                                    "C": "專注於簡化模型的結構以提高解釋性",
                                    "D": "主要用於無監督學習任務"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>集成學習 (Ensemble Learning) 是一種機器學習範式，其核心思想是「三個臭皮匠，勝過一個諸葛亮」。它不依賴於單一的、可能存在缺陷的模型，而是將多個相對簡單或性能較弱的學習器（稱為基學習器或弱學習器，weak learners）的預測結果以某種方式組合起來，從而創建一個整體性能更好、更穩健、泛化能力更強的強學習器 (strong learner)。常見的集成學習方法包括Bagging（如隨機森林Random Forest）、Boosting（如梯度提升樹Gradient Boosting Trees, AdaBoost）和Stacking等。</p><p><strong>(A) 錯誤。</strong>這與集成學習的思想相反。集成學習正是為了避免過度依賴單一模型，特別是當單一最強模型可能很難找到或容易過擬合時。</p><p><strong>(C) 錯誤。</strong>雖然某些集成方法（如隨機森林中的特徵重要性）可以提供一定的可解釋性，但集成學習的主要目標通常是提升預測性能和模型的穩健性，而不是簡化模型結構以提高解釋性。事實上，集成模型通常比單個基學習器更複雜，解釋性也可能更差。</p><p><strong>(D) 錯誤。</strong>集成學習方法主要應用於監督式學習任務（分類和迴歸），例如隨機森林和梯度提升樹都是監督式學習演算法。雖然集成思想也可以應用於非監督式學習（例如集成聚類），但其最典型和廣泛的應用是在監督式學習中。</p></div>"
                            }
                        ]
                    },
                    "L23202": {
                        "title": "常見機器學習演算法",
                        "questions": [
                            {
                                "question_text": "下列哪種演算法常用於解決二元分類 (Classification)問題，並透過一個S型函數 (Sigmoid function) 將線性輸出轉換 (Transform)為機率值？",
                                "options": {
                                    "A": "線性迴歸 (Linear Regression)",
                                    "B": "K-均值分群 (K-Means Clustering)",
                                    "C": "羅吉斯迴歸 (Logistic Regression)",
                                    "D": "主成分分析 (Principal Component Analysis)"
                                },
                                "correct_answer": "C",
                                "solution": "<div><p><strong>(C) 正確。</strong>羅吉斯迴歸 (Logistic Regression) 是一種廣泛應用於解決二元分類問題（即預測結果只有兩個類別，例如是/否、成功/失敗）的統計學習方法。儘管其名稱中帶有「迴歸」，但它本質上是一個分類演算法。羅吉斯迴歸首先計算輸入特徵的線性組合（類似於線性迴歸），然後將這個線性輸出透過一個S型函數（Sigmoid function，也稱為Logistic function）映射到0到1之間的值。這個值可以被解釋為樣本屬於某個正類別的機率。如果機率大於某個閾值（通常是0.5），則預測為正類別，否則預測為負類別。</p><p><strong>(A) 錯誤。</strong>線性迴歸 (Linear Regression) 主要用於解決迴歸問題，即預測一個連續的數值輸出，而不是分類問題。它直接輸出特徵的線性組合，不使用S型函數將其轉換為機率。</p><p><strong>(B) 錯誤。</strong>K-均值分群 (K-Means Clustering) 是一種非監督式學習演算法，用於將數據自動分組成K個不同的群體 (clusters)，它不需要標籤數據，也不是用於二元分類或輸出機率值。</p><p><strong>(D) 錯誤。</strong>主成分分析 (Principal Component Analysis, PCA) 是一種非監督式學習的降維技術，用於找到數據中方差最大的方向（主成分），並將數據投影到這些方向上以減少特徵數量。它不直接用於分類或輸出機率。</p></div>"
                            },
                            {
                                "question_text": "決策樹 (Decision Tree) 演算法在進行節點分裂時，通常會選擇哪個特徵？",
                                "options": {
                                    "A": "隨機選擇一個特徵",
                                    "B": "選擇能夠使得分裂後子節點的「純度」最高（例如，資訊增益最大或基尼不純度最小）的特徵",
                                    "C": "選擇數值範圍最大的特徵",
                                    "D": "選擇與目標變數相關性最小的特徵"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>決策樹 (Decision Tree) 是一種監督式學習演算法，它透過一系列的決策規則（以樹狀結構表示）來進行分類或迴歸。在構建決策樹的過程中，一個關鍵步驟是如何選擇最佳的特徵來分裂當前節點，以使得分裂後的子節點盡可能「純粹」（即子節點中的樣本盡可能屬於同一類別，或迴歸值盡可能相似）。常用的分裂標準包括：</p><p>-   <strong>資訊增益 (Information Gain)</strong>：基於熵 (Entropy) 的概念，選擇能夠最大程度減少分裂後子節點熵（不確定性）的特徵，即資訊增益最大的特徵。ID3演算法使用此標準。</p><p>-   <strong>增益率 (Gain Ratio)</strong>：對資訊增益進行校正，以避免偏向於選擇具有較多取值的特徵。C4.5演算法使用此標準。</p><p>-   <strong>基尼不純度 (Gini Impurity)</strong>：衡量從數據集中隨機選擇一個樣本，然後隨機分配一個標籤，該樣本被錯誤分類的機率。選擇能夠使得分裂後子節點基尼不純度最小（即純度最高）的特徵。CART (Classification and Regression Trees) 演算法使用此標準。</p><p>這些標準的核心思想都是選擇一個能夠最好地區分不同類別樣本的特徵進行分裂。</p><p><strong>(A) 錯誤。</strong>隨機選擇特徵進行分裂是某些集成學習方法（如隨機森林中的部分隨機性）或極端隨機樹 (Extremely Randomized Trees) 的做法，但標準的決策樹演算法（如ID3, C4.5, CART）在選擇分裂特徵時是有明確的優化目標的，而不是完全隨機。</p><p><strong>(C) 錯誤。</strong>特徵的數值範圍大小本身並不直接決定其對分類的貢獻。一個數值範圍小但區分能力強的特徵可能遠比一個數值範圍大但區分能力弱的特徵更適合用於分裂。</p><p><strong>(D) 錯誤。</strong>決策樹的目標是找到與目標變數最相關、最能區分不同目標值的特徵來進行分裂，而不是選擇相關性最小的特徵。選擇相關性最小的特徵會導致分類效果很差。</p></div>"
                            },
                            {
                                "question_text": "支持向量機 (Support Vector Machine, SVM) 在進行分類 (Classification)時，其主要目標是找到什麼？",
                                "options": {
                                    "A": "一個能夠穿過最多數據點的超平面",
                                    "B": "一個能夠將不同類別數據點分開，並且使得兩邊間隔 (margin) 最大的超平面",
                                    "C": "一個能夠將所有數據點都包含在內的最小圓形",
                                    "D": "數據點在低維空間中的最佳投影"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>支持向量機 (Support Vector Machine, SVM) 是一種強大的監督式學習演算法，常用於分類和迴歸任務。在分類任務中，SVM的核心思想是找到一個能夠將不同類別的數據點分隔開來的最佳決策邊界，這個決策邊界在二維空間中是一條直線，在更高維空間中則是一個超平面 (hyperplane)。SVM的「最佳」指的是這個超平面不僅能夠正確分開不同類別的數據，而且它到最近的來自兩個不同類別的數據點（這些點被稱為支持向量，support vectors）的距離（即間隔，margin）是最大的。最大化這個間隔有助於提高模型的泛化能力，使其對新的、未見過的數據有更好的分類效果。</p><p><strong>(A) 錯誤。</strong>SVM的目標不是找到穿過最多數據點的超平面。事實上，決策超平面通常不會穿過任何數據點（除非是線性不可分的情況下使用軟間隔）。</p><p><strong>(C) 錯誤。</strong>找到一個能夠將所有數據點都包含在內的最小圓形或球體，是與支持向量數據描述 (Support Vector Data Description, SVDD) 相關的概念，主要用於異常檢測或單類別分類，而不是標準SVM分類的主要目標。</p><p><strong>(D) 錯誤。</strong>數據點在低維空間中的最佳投影通常與降維技術相關，例如主成分分析 (PCA)。雖然SVM可以與核技巧 (kernel trick) 結合，將數據映射到更高維的特徵空間以實現非線性分類，但其核心目標仍然是找到最大間隔的分類超平面，而不是尋找最佳投影。</p></div>"
                            },
                            {
                                "question_text": "K-近鄰演算法 (K-Nearest Neighbors, KNN) 如何對新的數據點進行分類 (Classification)或預測？",
                                "options": {
                                    "A": "建立一個複雜的數學模型來描述數據分佈",
                                    "B": "找出訓練集中與新數據點最接近的K個鄰居，並根據這些鄰居的標籤進行投票或平均來決定新數據點的標籤",
                                    "C": "透過梯度下降 (Gradient Descent)法 (Gradient Descent)優化一個損失函數 (Loss Function)",
                                    "D": "將數據轉換 (Transform)到一個新的特徵空間"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>K-近鄰演算法 (K-Nearest Neighbors, KNN) 是一種簡單且直觀的非參數、懶惰學習 (lazy learning) 演算法，可用於分類和迴歸任務。其核心思想是「物以類聚」。當需要對一個新的、未標記的數據點進行預測時，KNN會執行以下步驟：</p><p>1.  計算新數據點與訓練集中所有已知數據點之間的距離（常用的距離度量包括歐幾里德距離、曼哈頓距離等）。</p><p>2.  選取距離新數據點最近的K個訓練數據點（即K個最近鄰居）。K是一個由使用者指定的超參數。</p><p>3.  對於分類問題，KNN會查看這K個鄰居的類別標籤，並採用「多數投票」的原則，將新數據點預測為K個鄰居中出現次數最多的那個類別。</p><p>4.  對於迴歸問題，KNN通常會計算這K個鄰居的目標值的平均數（或加權平均數），並將其作為新數據點的預測值。</p><p><strong>(A) 錯誤。</strong>KNN是一種非參數方法，它不對數據分佈做任何假設，也不會建立一個明確的數學模型來描述數據分佈（如羅吉斯迴歸或SVM那樣）。它直接依賴於訓練樣本本身進行預測。</p><p><strong>(C) 錯誤。</strong>KNN的訓練過程非常簡單，基本上只是儲存訓練數據。它不像許多其他機器學習演算法（如神經網路、羅吉斯迴歸）那樣需要透過梯度下降法等優化演算法來最小化一個損失函數以學習模型參數。</p><p><strong>(D) 錯誤。</strong>KNN直接在原始特徵空間中計算距離並進行預測，它本身不涉及將數據轉換到一個新的特徵空間。某些演算法（如SVM中的核技巧或PCA）會進行特徵空間轉換，但這不是KNN的核心機制。</p></div>"
                            },
                            {
                                "question_text": "下列哪種演算法屬於非監督式學習 (Unsupervised Learning)，常用於將數據集自動分組成若干個相似的群體 (clusters)？",
                                "options": {
                                    "A": "隨機森林 (Random Forest)",
                                    "B": "K-均值分群 (K-Means Clustering)",
                                    "C": "梯度提升機 (Gradient Boosting Machine)",
                                    "D": "樸素貝氏分類 (Classification)器 (Naive Bayes Classifier)"
                                },
                                "correct_answer": "B",
                                "solution": "<div><p><strong>(B) 正確。</strong>K-均值分群 (K-Means Clustering) 是一種經典且廣泛使用的非監督式學習演算法，其主要目的是將一個未標記的數據集劃分成K個互不相交的群體（或簇，clusters）。「非監督式」意味著算法在學習過程中不需要預先定義的類別標籤。K-Means透過迭代的方式，試圖最小化每個群體內數據點到該群體中心（質心，centroid）的平方距離之和，從而使得同一群體內的數據點盡可能相似，而不同群體之間的數據點盡可能不同。K是需要使用者預先指定的群體數量。</p><p><strong>(A) 錯誤。</strong>隨機森林 (Random Forest) 是一種集成學習方法，屬於監督式學習演算法，常用於分類和迴歸問題。它透過構建多個決策樹並結合它們的預測結果來提高模型的性能和穩健性。</p><p><strong>(C) 錯誤。</strong>梯度提升機 (Gradient Boosting Machine, GBM) 也是一種集成學習方法，屬於監督式學習演算法，常用於分類和迴歸問題。它透過迭代地訓練一系列弱學習器（通常是決策樹），每個新的學習器都試圖修正前面學習器的殘差，從而逐步提升整體模型的性能。</p><p><strong>(D) 錯誤。</strong>樸素貝氏分類器 (Naive Bayes Classifier) 是一種基於貝氏定理的監督式學習演算法，常用於分類問題。它假設特徵之間在給定類別的條件下是相互獨立的（樸素假設）。</p></div>"
                            },
                            {
                                "solution": "<div><p><strong>(B) 正確。</strong>在機器學習中，「參數」與「超參數」是兩個截然不同的概念：</p><p>- <strong>參數 (Parameters)</strong>：是模型內部的變量，在訓練過程中從數據中學習並優化得到的。例如，線性迴歸中的係數、神經網絡中的權重和偏置等。這些參數直接影響模型對新數據的預測結果，是通過最小化損失函數（如梯度下降）來自動求解的。</p><p>- <strong>超參數 (Hyperparameters)</strong>：是模型外部的設置，需要在訓練前由人工設定，用於控制學習算法本身的行為。超參數不是從數據中學習得到的，而是需要通過經驗、嘗試或系統性的調參技術（如網格搜索、隨機搜索）來確定。常見的超參數包括：學習率、正則化系數、神經網絡的層數和每層神經元數量、決策樹的最大深度、集成學習中的基學習器數量等。</p><p><strong>(A) 錯誤。</strong>該選項將參數和超參數的定義顛倒了。超參數是需要人工設定的，而不是從數據中學習得到的；參數則是模型在訓練過程中學習得到的，而不是手動設定的。</p><p><strong>(C) 錯誤。</strong>參數和超參數有本質的區別：一個是模型自動學習的，另一個是人工設定的；一個直接構成模型本身，另一個控制學習過程；它們絕不能互換使用。</p><p><strong>(D) 錯誤。</strong>超參數不僅存在於深度學習模型中，幾乎所有機器學習算法都有超參數。例如，決策樹的最大深度、支持向量機的核參數、K-近鄰的K值等，都是這些傳統機器學習算法的超參數。</p></div>",
                                "question_text": "在機器學習中，「超參數 (hyperparameters)」與「參數 (parameters)」的主要區別是什麼？",
                                "options": {
                                    "A": "超參數是模型從數據中學習的變量，參數是人工手動設定的變量",
                                    "B": "參數是模型從數據中學習的變量，超參數是在訓練前人工設定的變量",
                                    "C": "兩者本質上相同，可以互換使用",
                                    "D": "超參數僅在深度學習模型中存在，傳統機器學習模型只有參數"
                                },
                                "correct_answer": "B"
                            }
                        ]
                    },
                    "L23203": {
                        "title": "特徵工程與特徵選擇",
                        "questions": [
                            {
                                "question_text": "特徵工程 (Feature Engineering) 在機器學習中的重要性在於什麼？",
                                "options": {
                                    "A": "創建對模型效能有正面影響的新特徵，或轉換 (Transform)現有特徵，以更好地反映數據中的潛在規律",
                                    "B": "增加模型的複雜度以避免欠擬合 (Underfitting)",
                                    "C": "減少計算資源的使用",
                                    "D": "處理邏輯歸納的推論"
                                },
                                "correct_answer": "A",
                                "solution": "<div><p><strong>(A) 正確。</strong>特徵工程 (Feature Engineering) 是機器學習中非常重要的一個環節，它直接影響模型的性能。特徵工程的主要目的是創建對模型效能有正面影響的新特徵，或轉換現有特徵，以更好地表示和捕捉數據中的潛在規律和模式。好的特徵工程可以：</p><p>-   <strong>使模型更準確</strong>：如果特徵能夠很好地表達目標變量的相關信息，模型更容易學習到真實的模式。</p><p>-   <strong>提高訓練效率</strong>：好的特徵可以使模型更容易、更快速地學習到有用的模式。</p><p>-   <strong>增強模型的泛化能力</strong>：合適的特徵可以幫助模型更好地泛化到未見過的數據。</p><p>特徵工程的方法包括特徵選擇、特徵提取、特徵轉換（如歸一化、標準化）、特徵創建（如多項式特徵、交互特徵）等。</p><p><strong>(B) 錯誤。</strong>增加模型的複雜度可能有助於避免欠擬合，但這不是特徵工程的主要目的。特徵工程的重點是改善特徵以更好地表示數據中的關係，而非直接增加模型的複雜度。</p><p><strong>(C) 錯誤。</strong>良好的特徵工程可能會減少所需的計算資源（例如，通過特徵選擇減少特徵數量），但這通常是一個副作用，而非特徵工程的主要目的。特徵工程的主要目的是提高模型效能。</p><p><strong>(D) 錯誤。</strong>處理邏輯歸納的推論通常是推理系統或專家系統的任務，而非特徵工程的主要目的。特徵工程是將數據轉換為更適合機器學習模型的形式，以提高學習效率和模型性能。</p></div>"
                            },
                            {
                                "question_text": "特徵選擇 (Feature Selection) 的主要目的是什麼？",
                                "options": {
                                    "A": "增加訓練數據的數量",
                                    "B": "降低模型的泛化能力",
                                    "C": "選擇最相關的特徵以提升模型性能，降低複雜度，並減少過擬合 (Overfitting)",
                                    "D": "提高模型對噪聲 (noise)的敏感性"
                                },
                                "correct_answer": "C",
                                "solution": "<div><p><strong>(C) 正確。</strong>特徵選擇 (Feature Selection) 是指從原始特徵集中選擇出一個子集，以降低特徵維度並提升模型性能。其主要目的包括：</p><p>1.  <strong>選擇最相關的特徵</strong>：識別並保留那些與目標變量最相關、對預測結果最有影響力的特徵，同時排除不相關或冗餘的特徵。</p><p>2.  <strong>提升模型性能</strong>：通過去除噪聲特徵，模型可以集中學習真正相關的模式，從而提高預測準確性。</p><p>3.  <strong>降低模型複雜度</strong>：減少特徵數量可以簡化模型，降低計算復雜度，加快訓練和預測速度。</p><p>4.  <strong>減少過擬合風險</strong>：較少的特徵降低了模型的自由度，使其不太可能捕捉到訓練數據中的噪聲，從而提高模型在未見數據上的泛化能力。</p><p>5.  <strong>改善模型可解釋性</strong>：較少的特徵通常使模型更易於理解和解釋，尤其對於如線性模型或決策樹這樣的簡單模型。</p><p><strong>(A) 錯誤。</strong>特徵選擇不會增加訓練數據的數量，它是減少特徵數量的過程，而非增加樣本數量。增加訓練數據的方法包括收集更多數據、使用數據增強技術等。</p><p><strong>(B) 錯誤。</strong>特徵選擇的目的之一是通過去除不相關或冗餘特徵來減少過擬合，從而提高模型的泛化能力，而非降低泛化能力。</p><p><strong>(D) 錯誤。</strong>特徵選擇旨在選擇最相關、最有信息量的特徵，同時排除噪聲特徵，從而使模型對噪聲不那麼敏感，而非提高模型對噪聲的敏感性。</p></div>"
                            },
                            {
                                "question_text": "下列哪個不屬於特徵選擇 (Feature Selection)的常見方法？",
                                "options": {
                                    "A": "過濾法 (Filter Methods)",
                                    "B": "包裝法 (Wrapper Methods)",
                                    "C": "嵌入法 (Embedded Methods)",
                                    "D": "轉換 (Transform)維度法 (Dimension Transform Methods)"
                                },
                                "correct_answer": "D",
                                "solution": "<div><p><strong>(D) 正確。</strong>「轉換維度法 (Dimension Transform Methods)」不是特徵選擇的常見方法。實際上，這是一個牽強或混淆的術語，可能是指特徵提取或降維技術（如主成分分析 PCA 或 t-SNE），而非傳統意義上的特徵選擇。特徵選擇是指從原始特徵集中選擇出一個子集，保留原特徵而不創建新特徵；而降維（如 PCA）則是將高維特徵轉換為低維特徵，通常會創建新的特徵表示。特徵選擇的三種主要方法是過濾法、包裝法和嵌入法。</p><p><strong>(A) 錯誤。</strong>過濾法 (Filter Methods) 是一種特徵選擇方法，它基於特徵與目標變量之間的統計指標（如相關係數、互信息、卡方檢驗等）來評估每個特徵的重要性，然後選擇排名靠前的特徵。這類方法獨立於後續使用的學習算法，計算效率高，但可能無法捕捉到特徵之間的相互作用。</p><p><strong>(B) 錯誤。</strong>包裝法 (Wrapper Methods) 是一種特徵選擇方法，它使用目標機器學習算法的表現來評估特徵子集的好壞。它通過訓練模型來評估不同特徵子集的性能（如通過交叉驗證），然後選擇能使模型性能最優的特徵子集。常見的包裝法包括遞歸特徵消除 (Recursive Feature Elimination)、前向選擇 (Forward Selection)、後向消除 (Backward Elimination) 等。包裝法能夠捕捉特徵間的相互作用，但計算成本較高。</p><p><strong>(C) 錯誤。</strong>嵌入法 (Embedded Methods) 是一種特徵選擇方法，它將特徵選擇過程嵌入到模型訓練過程中。這些方法在模型學習的同時進行特徵選擇，通常是通過為模型參數增加懲罰項（如 L1 正則化，即 Lasso）來實現。嵌入法結合了過濾法的效率和包裝法的有效性，是一種較為平衡的方法。常見的嵌入法包括 Lasso 回歸、具有特徵選擇能力的樹模型等。</p></div>"
                            },
                            {
                                "question_text": "在數據預處理中，標準化 (Standardization) 和歸一化 (Normalization) 的主要區別是什麼？",
                                "options": {
                                    "A": "標準化 (Standardization)將數據轉換 (Transform)為標準常態分佈，歸一化 (Normalization)將數據縮放到特定範圍（通常是0到1之間）",
                                    "B": "兩者完全相同，是可互換的術語",
                                    "C": "標準化 (Standardization)用於分類 (Classification)任務，歸一化 (Normalization)用於迴歸 (Regression)任務",
                                    "D": "標準化 (Standardization)適用於離散型特徵，歸一化 (Normalization)適用於連續型特徵"
                                },
                                "correct_answer": "A",
                                "solution": "<div><p><strong>(A) 正確。</strong>標準化 (Standardization) 和歸一化 (Normalization) 是兩種不同的數據預處理技術，它們的主要區別在於轉換後數據的性質：</p><p>-   <strong>標準化 (Standardization)</strong>：又稱為 Z-score 正規化，它將數據轉換為均值為0、標準差為1的標準常態分佈。標準化後的數據沒有確定的範圍，理論上可以取任何值。標準化公式為：</p><p>    Z = (X - μ) / σ</p><p>    其中，X 是原始數據，μ 是數據的均值，σ 是數據的標準差。</p><p>-   <strong>歸一化 (Normalization)</strong>：也稱為 Min-Max 縮放，它將數據線性轉換到一個特定的範圍內，通常是 [0, 1] 或 [-1, 1]。歸一化公式為：</p><p>    X_norm = (X - X_min) / (X_max - X_min)</p><p>    其中，X 是原始數據，X_min 和 X_max 分別是數據的最小值和最大值。</p><p><strong>(B) 錯誤。</strong>標準化和歸一化不是完全相同的術語，它們代表了不同的數據轉換技術，如上所述。</p><p><strong>(C) 錯誤。</strong>標準化和歸一化的適用情況與任務類型（分類或迴歸）沒有直接對應關係。選擇哪種方法通常取決於數據的性質、算法的需求以及特定的問題背景。例如，梯度下降通常對歸一化後的數據效果更好；主成分分析 (PCA) 通常需要標準化的數據；對於基於距離的算法如 K-近鄰或 K-均值，歸一化可以確保不同特徵的距離計算公平。</p><p><strong>(D) 錯誤。</strong>標準化和歸一化都主要適用於連續型特徵。對於離散型特徵，特別是無序的類別型特徵，通常使用其他方法如獨熱編碼 (One-Hot Encoding) 來處理。標準化和歸一化不是基於特徵的數據類型（離散或連續）來區分的。</p></div>"
                            },
                            {
                                "question_text": "下列哪個不屬於有效的特徵工程 (Feature Engineering)技術？",
                                "options": {
                                    "A": "主成分分析 (Principal Component Analysis, PCA)",
                                    "B": "獨熱編碼 (One-Hot Encoding)",
                                    "C": "多項式特徵創建 (Polynomial Feature Creation)",
                                    "D": "降低訓練數據量 (Reducing Training Data)"
                                },
                                "correct_answer": "D",
                                "solution": "<div><p><strong>(D) 正確。</strong>「降低訓練數據量」不是特徵工程的一種技術，相反，它可能損害模型的性能。更多的訓練數據通常有助於模型學習更準確的模式和關係，而減少數據量可能導致模型欠擬合或無法捕捉足夠的數據變異性。特徵工程是關於如何轉換或選擇特徵以提高模型性能，而不是減少訓練數據量。</p><p><strong>(A) 錯誤。</strong>主成分分析 (PCA) 是一種特徵提取和降維技術，它將原始特徵轉換為一組新的、互相正交的特徵（稱為主成分），並按其解釋的數據變異量排序。通過選擇前k個主成分，可以在保留大部分數據信息的前提下降低特徵維度，這確實是特徵工程的一種有效技術。</p><p><strong>(B) 錯誤。</strong>獨熱編碼 (One-Hot Encoding) 是一種處理類別型特徵的技術，它將一個類別型特徵轉換為多個二元特徵。例如，如果一個特徵有三個可能的類別值 (A, B, C)，獨熱編碼會將其轉換為三個特徵 (Is_A, Is_B, Is_C)，每個特徵的值是 0 或 1。這是特徵工程中處理類別型數據的標準方法。</p><p><strong>(C) 錯誤。</strong>多項式特徵創建 (Polynomial Feature Creation) 是一種特徵工程技術，它通過創建原始特徵的多項式組合（如平方項、交叉項等）來捕捉特徵之間的非線性關係。例如，如果原始特徵是 x1 和 x2，多項式特徵可能包括 x1^2, x1*x2, x2^2 等。這種技術可以增強模型捕捉複雜關係的能力，尤其是對於線性模型而言。</p></div>"
                            }
                        ]
                    }
                }
            }
        }
    }
}